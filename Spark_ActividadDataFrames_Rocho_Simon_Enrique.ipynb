{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmh-0M-EGUGt"
   },
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# PEC 2: Parte 2, Marzo 2023\n",
    "\n",
    "## Extracción de conocimiento de fuentes de datos heterogéneas mediante Spark SQL, RDDs y GraphFrames\n",
    "\n",
    "En esta práctica vamos a introducir estructuras de datos más complejas que las vistas hasta ahora, donde los campos pueden a su vez tener campos anidados. En concreto utilizaremos datos de twitter capturados en el contexto de las elecciones generales en España del 28 de Abril de 2019. La práctica está estructurada de la siguiente manera:\n",
    "- **Parte 0:** Configuración del entorno\n",
    "- **Parte 1:** Introducción a data frames estructurados y cómo operar extraer información *(3 puntos)*\n",
    "    - **Parte 1.1:** Importar los datos *(1 puntos)*\n",
    "    - **Parte 1.2:** *Queries* sobre sobre data frames complejos *(2 puntos)*\n",
    "        - **Parte 1.2.1:** Queries SQL *(1 puntos)*\n",
    "        - **Parte 1.2.2:** Queries sobre el pipeline *(1 puntos)*\n",
    "- **Parte 2:** Bases de datos HIVE y operaciones complejas *(3 puntos)*\n",
    "    - **Parte 2.1:** Bases de datos Hive *(1 puntos)*\n",
    "    - **Parte 2.2:** Más allá de las transformaciones SQL *(2 puntos)*\n",
    "        - **Parte 2.2.1:** Tweets por población  *(1 puntos)*\n",
    "        - **Parte 2.2.2:** Contar hashtags *(1 puntos)*\n",
    "- **Parte 3:** Sampling *(2 Puntos)*\n",
    "- **Parte 4**: Introducción a los datos relacionales *(2 puntos)*\n",
    "     - **Parte 4.1**: Construcción de la edgelist *(1 puntos)*\n",
    "     - **Parte 4.2**: Centralidad de grado *(1 puntos)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ASnskRaGUGx"
   },
   "source": [
    "## **Parte 0:** Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import random\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master=\"local[1]\", appName=\"PEC3_erocho\")\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2qE1sQRGUG0",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 1:** Introducción a data frames estructurados y operaciones sobre ellos.\n",
    "\n",
    "Como ya se ha mencionado, en esta práctica vamos a utilizar datos de Twitter que recolectamos durante las elecciones generales en España del 28 de abril de 2019. Como veremos, los tweets tienen una estructura interna bastante compleja que hemos simplificado un poco en esta práctica.\n",
    "\n",
    "### **Parte 1.1:** Importar los datos\n",
    "\n",
    "Lo primero que vamos ha aprender es cómo importar este tipo de datos a nuestro entorno. Uno de los tipos de archivos más comunes para guardar este formato de información es [la estructura JSON](https://en.wikipedia.org/wiki/JSON). Esta estructura permite guardar información en un texto plano de diferentes objetos siguiendo una estructura de diccionario donde cada campo tiene asignado una llave y un valor. La estructura puede ser anidada, o sea que una llave puede tener como valor otra estructura tipo diccionario.\n",
    "\n",
    "Spark SQL permite leer datos de muchos formatos diferentes (como recordareis de la anterior práctica donde leímos un fichero CSV). En esta ocasión, se os pide que leáis un fichero JSON de la ruta ```/aula_22.419/data/tweets28a_sample.json```. Este archivo contiene un pequeño *sample*, un 0.1% de la base de datos completa (en un siguiente apartado veremos cómo realizar este *sampleado*). En esta ocasión no se os pide especificar la estructura del data frame ya que la función de lectura la inferirá automáticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3R3EQBVYGUG0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 27268 tweets\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "tweets_sample = sqlContext.read.json(\"/aula_22.419/data/tweets28a_sample.json\")\n",
    "\n",
    "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj93OIWeGUG1"
   },
   "source": [
    "El siguiente paso es mostrar la estructura del dataset que acabamos de cargar. Recordad que podéis obtener la información acerca de cómo está estructurado el DataTable utilizando el método ```printSchema()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9sL3masaGUG1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv0DNkyYGUG2"
   },
   "source": [
    "Podéis observar que la estructura del tweet contiene múltiples campos anidados. Teneis que familiarizaros con esta estructura ya que será la que utilizaremos durante toda la práctica. Recordad también que no todos los tweets tienen todos los campos, como por ejemplo la ubicación (campo ```place```). Cuando esto pasa el campo pasa a ser ```NULL```. Podéis ver mas información sobre este tipo de datos en [este enlace](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tvG-wtZGUG2",
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 1.2:** *Queries* sobre sobre data frames complejos\n",
    "\n",
    "En esta parte vamos a introducir conceptos sobre cómo trabajar con data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfQmHucCGUG2"
   },
   "source": [
    "#### **Parte 1.2.1:** Queries SQL\n",
    "\n",
    "El primer paso consiste en registrar la tabla en el contexto SQL comprobando primero si existe y borrándola en el caso que sea así. En este apartado se os pide que registréis la tabla ```tweets_sample``` que acabamos de cargar en el contexto sql bajo el mismo nombre ```tweets_sample```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eld-cll_GUG2"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: tweets_sample; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: tweets_sample; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:651)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'tweets_sample' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:84)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:120)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:746)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:146)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 73 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4cf1d7a4fc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Primero comprobamos si hay algo para la tabla tweets_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from tweets_sample LIMIT 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: tweets_sample; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "#Primero comprobamos si hay algo para la tabla tweets_sample\n",
    "sqlContext.sql(\"select * from tweets_sample LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registramos\n",
    "sqlContext.registerDataFrameAsTable(tweets_sample, \"tweets_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='1122569971345833984', created_at=1556476512, lang='und', place=None, retweeted_status=Row(_id='1122566927866634245', user=Row(followers_count=3109, friends_count=2978, id_str='301428215', lang='es', screen_name='Ibero_DMJ', statuses_count=52713)), text='RT @Ibero_DMJ: 😂😂😂😂😂 https://t.co/GkuSJPpboj', user=Row(followers_count=2511, friends_count=4383, id_str='1069389675645870080', lang='en', screen_name='jhlacasa1', statuses_count=3360)),\n",
       " Row(_id='1122570131736006656', created_at=1556476550, lang='pt', place=None, retweeted_status=None, text='@CervantesFAQs @Cazatalentos @vox_es @Irene_Montero_ Asco de gente y de V💩X!', user=Row(followers_count=33, friends_count=119, id_str='1030176768056012801', lang='es', screen_name='TeresaAngelin15', statuses_count=1838)),\n",
       " Row(_id='1122570215445999617', created_at=1556476570, lang='es', place=None, retweeted_status=Row(_id='1122561071426998273', user=Row(followers_count=1101665, friends_count=415, id_str='26729931', lang='es', screen_name='rtve', statuses_count=198717)), text='RT @rtve: El PSOE ganaría las elecciones y necesitaría a los independentistas para gobernar, según el sondeo de Gad3 para RTVE  https://t.c…', user=Row(followers_count=1741, friends_count=1498, id_str='250333298', lang='es', screen_name='LidiaSierraBUR', statuses_count=14759)),\n",
       " Row(_id='1122570274610851846', created_at=1556476584, lang='es', place=None, retweeted_status=None, text='#SiguemeYTeSigoVox voy a intentar volver a tener una cuenta de política aunque la izquierda me haya censurado ya va… https://t.co/5ujCE1bgPt', user=Row(followers_count=71, friends_count=243, id_str='1079050390472216576', lang='es', screen_name='DanielJ84562737', statuses_count=565)),\n",
       " Row(_id='1122570283129479169', created_at=1556476586, lang='es', place=None, retweeted_status=Row(_id='1122541491895832577', user=Row(followers_count=493, friends_count=695, id_str='193802591', lang='es', screen_name='Aljiuss', statuses_count=14700)), text='RT @Aljiuss: Ahí lo teneis, Rafael Martínez 105 años. En 1936 votó al Frente Popular y en 2019 votando a Unidas Podemos.... Ahora todas y t…', user=Row(followers_count=1, friends_count=31, id_str='1122490258791260160', lang='en', screen_name='Elen29842466', statuses_count=2)),\n",
       " Row(_id='1122570300904878080', created_at=1556476590, lang='es', place=Row(bounding_box=Row(coordinates=[[[-5.274768, 37.080994], [-5.274768, 37.39277], [-4.965451, 37.39277], [-4.965451, 37.080994]]], type='Polygon'), country_code='ES', id='f33076431b70b3a3', name='Osuna', place_type='city'), retweeted_status=None, text='Ejemplos de porque la prensa pierde credibilidad a diario', user=Row(followers_count=1130, friends_count=1029, id_str='555078632', lang='es', screen_name='JUANDEDIOSFISIO', statuses_count=6379)),\n",
       " Row(_id='1122570301940871168', created_at=1556476590, lang='pt', place=None, retweeted_status=Row(_id='1122300505810714625', user=Row(followers_count=106964, friends_count=972, id_str='43340387', lang='en', screen_name='DCM_online', statuses_count=100332)), text='RT @DCM_online: Pesquisa Vox Populi: Lula 10 x Bolsonaro 1 https://t.co/su98bVMNu4', user=Row(followers_count=177, friends_count=372, id_str='77369029', lang='pt', screen_name='SpinaOficial', statuses_count=30608)),\n",
       " Row(_id='1122570422178938880', created_at=1556476619, lang='und', place=None, retweeted_status=None, text='😂😂😂😂😂😂😂😂', user=Row(followers_count=5808, friends_count=507, id_str='139452633', lang='es', screen_name='sergiofmarca', statuses_count=25654)),\n",
       " Row(_id='1122570463962595334', created_at=1556476629, lang='es', place=None, retweeted_status=Row(_id='1122570189760028673', user=Row(followers_count=205, friends_count=187, id_str='1055192007986028546', lang='es', screen_name='Oktubrenoticias', statuses_count=841)), text='RT @Oktubrenoticias: Las encuestas de GAD3 y COPE dan al PSOE la posibilidad de gobernar con el apoyo de Unidas Podemos e independentistas…', user=Row(followers_count=99, friends_count=68, id_str='1057047241859444736', lang='es', screen_name='eligiendouy', statuses_count=37)),\n",
       " Row(_id='1122570610381660160', created_at=1556476664, lang='es', place=None, retweeted_status=Row(_id='1122523272300548096', user=Row(followers_count=17605, friends_count=641, id_str='337731568', lang='es', screen_name='_iMperfectB', statuses_count=46610)), text='RT @_iMperfectB: Los que tenéis familiares que votan a VOX y soportáis sus comentarios e ideales de mierda prácticamente a diario porque no…', user=Row(followers_count=543, friends_count=426, id_str='1557641346', lang='es', screen_name='xlbxmxnz', statuses_count=20388))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisamos ahora si hay info de nuevo:\n",
    "sqlContext.sql(\"select * from tweets_sample LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjbyKTvOGUG3"
   },
   "source": [
    "Ahora se os pide que creeis una tabla ```users_agg``` con [la información agregada](https://www.w3schools.com/sql/sql_groupby.asp) de los usuarios que tengan definido su idioma (```user.lang```) como español (```es```). En concreto se os pide que la tabla contenga las siguientes columnas:\n",
    "- **screen_name:** nombre del usuario\n",
    "- **friends_count:** número máximo (ver nota) de personas a las que sigue\n",
    "- **tweets:** número de tweets realizados\n",
    "- **followers_count:** número máximo (ver nota) personas que siguen al usuario.\n",
    "\n",
    "El orden en el cual se deben mostrar los registros es orden descendente acorde al número de tweets.\n",
    "\n",
    "***Nota:*** es importante que os fijéis que el nombre de *friends* i *followers* puede diferir a lo largo de la adquisición de datos. En este caso vamos ha utilizar la función de agregación ```MAX``` sobre cada uno de estos campos para evitar segmentar el usuario en diversas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M0MmT_DBGUG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  PabloChabolas|         4925|     9|           4042|\n",
      "|      lolalailo|         4922|     9|           3738|\n",
      "|     Lordcrow11|         5002|     9|           3069|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg = sqlContext.sql(\"\"\"  SELECT user.screen_name, MAX(user.friends_count) AS friends_count, COUNT (_id) as tweets, MAX(user.followers_count) AS followers_count\n",
    "                                FROM tweets_sample\n",
    "                                WHERE user.lang == 'es'\n",
    "                                GROUP BY user.screen_name\n",
    "                                ORDER BY tweets DESC\"\"\")\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "j-ECTvkjGUG3"
   },
   "outputs": [],
   "source": [
    "output = users_agg.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "551-Szm6GUG4"
   },
   "source": [
    "Imaginad ahora que queremos combinar la información que acabamos de generar con información acerca del número de veces que un usuario ha sido retuiteado. Para hacer este tipo de combinaciones necesitamos recurrir al [```JOIN``` de tablas](https://www.w3schools.com/sql/sql_join.asp). Primero debemos registrar la tabla que acabamos de generar en el contexto SQL. Recordad que primero debéis comprobar si la tabla existe y en caso afirmativo eliminarla ('drop table if exists <tabla>'). La tabla tenéis que registrarla bajo el nombre de ```user_agg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RUrrffcJGUG4"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: user_agg; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: user_agg; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:651)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'user_agg' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:84)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:120)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:746)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:146)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 73 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dfb9f96b489e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from user_agg LIMIT 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: user_agg; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "#Revisamos si existe:\n",
    "sqlContext.sql(\"select * from user_agg LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registramos:\n",
    "sqlContext.registerDataFrameAsTable(users_agg,\"user_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(screen_name='anaoromi', friends_count=6258, tweets=16, followers_count=6774),\n",
       " Row(screen_name='RosaMar6254', friends_count=6208, tweets=14, followers_count=6245),\n",
       " Row(screen_name='lyuva26', friends_count=3088, tweets=13, followers_count=3732),\n",
       " Row(screen_name='PisandoFuerte10', friends_count=2795, tweets=12, followers_count=1752),\n",
       " Row(screen_name='carrasquem', friends_count=147, tweets=12, followers_count=215),\n",
       " Row(screen_name='jasalo54', friends_count=1889, tweets=11, followers_count=689),\n",
       " Row(screen_name='lolalailo', friends_count=4922, tweets=9, followers_count=3738),\n",
       " Row(screen_name='Lordcrow11', friends_count=5002, tweets=9, followers_count=3069),\n",
       " Row(screen_name='DuroBelinda', friends_count=5242, tweets=9, followers_count=5778),\n",
       " Row(screen_name='PabloChabolas', friends_count=4925, tweets=9, followers_count=4042)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisamos de nuevo\n",
    "sqlContext.sql(\"select * from user_agg LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwLsGWbeGUG4"
   },
   "source": [
    "Una vez registrada se pide que combinéis esta tabla y la tabla ```tweets_sample``` utilizando un ```INNER JOIN``` para obtener una nueva tabla con la siguiente información:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***friends_count:*** número máximo de personas a las que sigue\n",
    "- ***followers_count:*** número máximo de personas que siguen al usuario.\n",
    "- ***tweets:*** número de tweets realizados por el usuario.\n",
    "- ***retweeted:*** número de retweets obtenidos por el usuario.\n",
    "- ***ratio_tweet_retweeted:*** ratio de retweets por número de tweets publicados $\\frac{retweets}{tweets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I4RPdH8qGUG4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|   screen_name|friends_count|followers_count|tweets|retweeted|ratio_tweet_retweeted|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|          PSOE|        13635|         671073|     1|      155|                155.0|\n",
      "|  CiudadanosCs|        92910|         511896|     1|      117|                117.0|\n",
      "|     JuntsXCat|          202|          88515|     1|       73|                 73.0|\n",
      "|  PartidoPACMA|         1498|         232932|     1|       63|                 63.0|\n",
      "|  pablocasado_|         4567|         238926|     1|       50|                 50.0|\n",
      "|voxnoticias_es|         2146|          29582|     1|       44|                 44.0|\n",
      "|RaiLopezCalvet|         7579|          13574|     1|       43|                 43.0|\n",
      "|        iunida|        10225|         558318|     1|       39|                 39.0|\n",
      "|        Xuxipc|          311|         184967|     1|       37|                 37.0|\n",
      "|       Panik81|         1587|          15374|     1|       29|                 29.0|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = sqlContext.sql(\"\"\" SELECT user_agg.screen_name, user_agg.friends_count, user_agg.followers_count, user_agg.tweets,COUNT(*) as retweeted, COUNT(*) / user_agg.tweets AS ratio_tweet_retweeted\n",
    "                                FROM user_agg\n",
    "                                INNER JOIN tweets_sample\n",
    "                                WHERE user_agg.screen_name = retweeted_status.user.screen_name\n",
    "                                GROUP BY user_agg.screen_name, user_agg.friends_count, user_agg.followers_count, user_agg.tweets\n",
    "                                ORDER BY ratio_tweet_retweeted DESC\"\"\")\n",
    "\n",
    "retweeted.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ixMKO0CsGUG5"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Co-KvEGUG5"
   },
   "source": [
    "#### **Parte 1.2.2:** Queries a través de la API\n",
    "\n",
    "Las tablas de Spark SQL ofrecen otro mecanismo para aplicar las transformaciones y obtener resultados similares a los que se obtendría aplicando una consulta SQL. Por ejemplo utilizando el siguiente pipeline obtendremos el texto de todos los tweets en español:\n",
    "\n",
    "```\n",
    "tweets_sample.where(\"lang == 'es'\").select(\"text\")\n",
    "```\n",
    "\n",
    "Que es equivalente a la siguiente sentencia SQL:\n",
    "\n",
    "```\n",
    "SELECT text\n",
    "FROM tweets_sample\n",
    "WHERE lang == 'es'\n",
    "```\n",
    "\n",
    "Podéis consultar el [API de spark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html) para encontrar más información sobre como utilitzar las diferentes transformaciones en tablas.\n",
    "\n",
    "En este ejercicio se os pide que repliquéis la query obtenida en el apartado anterior empezando por generar la tabla ```users_agg```. Podéis utilizar las transformaciones ```where```, ```select``` (o ```selectExpr```), ```groupBy```, ```count```, ```agg``` y ```orderBy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_IMzsk98GUG7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+--------------------+\n",
      "|    screen_name|max(friends_count)|count(screen_name)|max(followers_count)|\n",
      "+---------------+------------------+------------------+--------------------+\n",
      "|       anaoromi|              6258|                16|                6774|\n",
      "|    RosaMar6254|              6208|                14|                6245|\n",
      "|        lyuva26|              3088|                13|                3732|\n",
      "|PisandoFuerte10|              2795|                12|                1752|\n",
      "|     carrasquem|               147|                12|                 215|\n",
      "|       jasalo54|              1889|                11|                 689|\n",
      "|  PabloChabolas|              4925|                 9|                4042|\n",
      "|      lolalailo|              4922|                 9|                3738|\n",
      "|     Lordcrow11|              5002|                 9|                3069|\n",
      "|    DuroBelinda|              5242|                 9|                5778|\n",
      "+---------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = tweets_sample.where(\"user.lang == 'es'\").select(\"user.screen_name\",\"user.friends_count\",\"user.followers_count\")\n",
    "\n",
    "users_agg = users.groupBy(\"screen_name\")\\\n",
    "                 .agg({\"screen_name\":\"count\", \"followers_count\":\"max\", \"friends_count\":\"max\"})\\\n",
    "                 .orderBy(\"count(screen_name)\", ascending=False)\n",
    "\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGAzPr-MGUG7"
   },
   "source": [
    "Si os fijáis veréis que el nombre de las columnas no corresponde con el obtenido anteriormente, podéis cambiar el nombre de una columna determinada utilizando la transformación ```withColumnRenamed```. Cambiad el nombre de las columnas para que coincidan con el apartado anterior y guardadlas en una variable ```user_agg_new```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XrFU4iP6GUG7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  PabloChabolas|         4925|     9|           4042|\n",
      "|      lolalailo|         4922|     9|           3738|\n",
      "|     Lordcrow11|         5002|     9|           3069|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg_new = users_agg.withColumnRenamed(\"max(friends_count)\", \"friends_count\")\\\n",
    "                         .withColumnRenamed(\"count(screen_name)\", \"tweets\")\\\n",
    "                         .withColumnRenamed(\"max(followers_count)\", \"followers_count\")\n",
    "\n",
    "users_agg_new.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dgU_U8vWGUG7"
   },
   "outputs": [],
   "source": [
    "output = users_agg_new.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX3nUTbdGUG7"
   },
   "source": [
    "Cread ahora una tabla ```user_retweets``` utilizando transformaciones que contenga dos columnas:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***retweeted:*** número de retweets\n",
    "\n",
    "Podéis utilizar las mismas transformaciones que en el ejercicio anterior. Ordenad la tabla en orden descendente utilizando el valor de la columna ```retweeted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|    screen_name|retweeted|\n",
      "+---------------+---------+\n",
      "|         vox_es|      299|\n",
      "|  Santi_ABASCAL|      238|\n",
      "|   ahorapodemos|      238|\n",
      "|       iescolar|      166|\n",
      "|  AlbanoDante76|      161|\n",
      "|           PSOE|      155|\n",
      "| AntonioMaestre|      154|\n",
      "|         boye_g|      142|\n",
      "|   CiudadanosCs|      117|\n",
      "|Pablo_Iglesias_|      108|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_retweets = tweets_sample.select(\"retweeted_status.user.screen_name\", \"retweeted_status._id\") \\\n",
    "    .filter(\"retweeted_status.user.lang = 'es'\") \\\n",
    "    .groupBy(\"screen_name\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"retweeted\") \\\n",
    "    .orderBy(\"retweeted\", ascending=False)\n",
    "\n",
    "user_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Q8Idkd_jGUG8"
   },
   "outputs": [],
   "source": [
    "output = user_retweets.first()\n",
    "assert output.screen_name == 'vox_es' and output.retweeted == 299, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nmAyq16GUG8"
   },
   "source": [
    "Otra manera de combinar dos tablas es utilizando el [metodo de tabla ```join```](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html). Combinad la información de la tabla ```users_agg_new``` y ```user_retweets``` en una nueva tabla ```retweeted``` utilizando la columna ```screen_name```. Ordenad la nueva tabla en orden descendente con el nombre de retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dLtDccNbGUG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "|   screen_name|friends_count|tweets|followers_count|   screen_name|retweeted|\n",
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "|          PSOE|        13635|     1|         671073|          PSOE|      155|\n",
      "|  CiudadanosCs|        92910|     1|         511896|  CiudadanosCs|      117|\n",
      "|     JuntsXCat|          202|     1|          88515|     JuntsXCat|       73|\n",
      "|  PartidoPACMA|         1498|     1|         232932|  PartidoPACMA|       63|\n",
      "|  pablocasado_|         4567|     1|         238926|  pablocasado_|       50|\n",
      "|voxnoticias_es|         2146|     1|          29582|voxnoticias_es|       44|\n",
      "|RaiLopezCalvet|         7579|     1|          13574|RaiLopezCalvet|       43|\n",
      "|        iunida|        10225|     1|         558318|        iunida|       39|\n",
      "|        Xuxipc|          311|     1|         184967|        Xuxipc|       37|\n",
      "|       Panik81|         1587|     1|          15374|       Panik81|       29|\n",
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = users_agg_new.join(user_retweets,users_agg_new.screen_name == user_retweets.screen_name, 'inner')\\\n",
    "                         .orderBy(\"retweeted\", ascending = False)\n",
    "\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BPI1fQ8QGUG8"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmdB3_lNGUG8"
   },
   "source": [
    "Notaréis que algunos de los registros que aparecen en la tabla ```users_retweeted``` no están presentes en la tabla retweeted. Esto es debido a que, por defecto, el método aplica un inner join y por tanto solo combina los registros presentes en ambas tablas. Podéis cambiar este comportamiento a través de los parámetros de la función.\n",
    "\n",
    "Para terminar esta parte y reconstruir el resultado del apartado 1.2.1 vamos a añadir una columna ```ratio_tweet_retweeted``` con información del ratio entre retweets y tweets. Para ello debéis utilizar la transformación ```withColumn```. El resultado debe estar ordenado considerando esta nueva columna en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aZPkN3TpGUG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "|   screen_name|friends_count|tweets|followers_count|   screen_name|retweeted|ratio_tweet_retweeted|\n",
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "|          PSOE|        13635|     1|         671073|          PSOE|      155|                155.0|\n",
      "|  CiudadanosCs|        92910|     1|         511896|  CiudadanosCs|      117|                117.0|\n",
      "|     JuntsXCat|          202|     1|          88515|     JuntsXCat|       73|                 73.0|\n",
      "|  PartidoPACMA|         1498|     1|         232932|  PartidoPACMA|       63|                 63.0|\n",
      "|  pablocasado_|         4567|     1|         238926|  pablocasado_|       50|                 50.0|\n",
      "|voxnoticias_es|         2146|     1|          29582|voxnoticias_es|       44|                 44.0|\n",
      "|RaiLopezCalvet|         7579|     1|          13574|RaiLopezCalvet|       43|                 43.0|\n",
      "|        iunida|        10225|     1|         558318|        iunida|       39|                 39.0|\n",
      "|        Xuxipc|          311|     1|         184967|        Xuxipc|       37|                 37.0|\n",
      "|       Panik81|         1587|     1|          15374|       Panik81|       29|                 29.0|\n",
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = retweeted.withColumn(\"ratio_tweet_retweeted\", retweeted[\"retweeted\"] / retweeted[\"tweets\"]).orderBy(\"ratio_tweet_retweeted\", ascending=False)\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pKmQvo-dGUG9"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QvvaxglGUG9",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 2:** Bases de datos HIVE y operaciones complejas\n",
    "\n",
    "Hasta ahora hemos estado trabajando con un pequeño sample de los tweets generados (el 0.1%). En esta parte de la PEC vamos a ver como trabajar y tratar con el dataset completo. Para ello vamos a utilizar tanto transformaciones sobre tablas como operaciones sobre RDD cuando sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSfN7McGUG9"
   },
   "source": [
    "### **Parte 2.1:** Bases de datos Hive\n",
    "\n",
    "Muchas veces los datos con los que vamos a trabajar se van a utilizar en diversos proyectos. Una manera de organizar los datos es, en lugar de utilizar directamente los ficheros, recurrir a una base de datos para gestionar la información. En el entorno Hadoop una de las bases de datos más utilizadas es [Apache Hive](https://hive.apache.org/), una base de datos que permite trabajar con contenido distribuido.\n",
    "\n",
    "La manera de acceder a esta base de datos es creando un contexto Hive de manera muy similar a como declaramos un contexto SQL. Primero de todo vamos a declarar un variable ```hiveContext``` instanciándola como un objeto de la classe ```HiveContext```. Acto seguido vamos a comprobar cuantas tablas están registradas en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4uUhHlKXGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "| default|               as|      false|\n",
      "| default|        mistweets|      false|\n",
      "| default|     province_28a|      false|\n",
      "| default|province_28a_vext|      false|\n",
      "| default|              src|      false|\n",
      "| default|        tweets28a|      false|\n",
      "| default|   tweets28a_vext|      false|\n",
      "| default|        user_info|      false|\n",
      "| default|   user_info_vext|      false|\n",
      "|        |    tweets_sample|       true|\n",
      "|        |         user_agg|       true|\n",
      "+--------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext = HiveContext(sc)\n",
    "hiveContext.tables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT5Lzi2dGUG9"
   },
   "source": [
    "Observad que ahora mismo tenemos cinco tablas registradas en este contexto. Tres de ellas no temporales y dos temporales, las que hemos registrado previamente. Por tanto sqlContext y hiveContext están concetados (es la misma sessión)\n",
    "\n",
    "Vamos a crear una variable ```tweets``` que utilizaremos para acceder a la tabla ```tweets28a``` guardada en ```hiveContext``` utilizando para ello el método ```table()``` de este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "psEvAQUEGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 25419835 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = hiveContext.table(\"tweets28a\")\n",
    "print(\"Loaded dataset contains {} tweets\".format(tweets.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGlfGwuHGUG9"
   },
   "source": [
    "Utilizando el mismo método que en el apartado 1.1, comprobad la estructura de la tabla que acabamos de cargar con ```printSchema()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SC61HsFsGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTDFYQzlGUG-",
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 2.2:** Más allá de las transformaciones SQL\n",
    "\n",
    "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van más allá de lo que podemos conseguir utilizando el lenguaje SQL. En esta parte de la práctica vamos practicar cómo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyzhlfUFGUG-",
    "toc-hr-collapsed": false
   },
   "source": [
    "#### **Parte 2.2.1:** Tweets por población\n",
    "\n",
    "Un pequeño porcentaje, alrededor del 1%, de los tweets realizados está geolocalizado. Eso quiere decir que para estos tweets tenemos información acerca del lugar donde han sido realizados guardado en el campo ```place```. En este ejercicio se pide que utilizando una sentencia SQL mostréis en orden descendente cuántos tweets se han realizado en cada lugar. La tabla resultante ```tweets_place``` debe tener las siguientes columnas:\n",
    "- ***name:*** nombre del lugar\n",
    "- ***tweets:*** número de tweets\n",
    "\n",
    "Recordad que no todos los tweets en la base de datos tienen que tener información geolocalizada, tenéis que filtrarlos teniendo en cuenta todos los que tienen un valor no nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kOMIut4rGUG-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|Villamartín|  2364|\n",
      "|     Málaga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = hiveContext.sql(\"\"\"  SELECT place.name, Count (*) as tweets\n",
    "                                    FROM tweets28a\n",
    "                                    WHERE place IS NOT NULL\n",
    "                                    GROUP BY place.name\n",
    "                                    ORDER BY tweets DESC\"\"\")\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nbT2HdsMGUG-"
   },
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hTeGFtnGUG-"
   },
   "source": [
    "#### **Parte 2.2.2:** Contar hashtags\n",
    "\n",
    "Como seguro que habéis observado la semántica de la sentencia SQL es mucho más limpia que trabajar con RDDs para realizar muchas tareas, pero no todas las que os vais a encontrar se pueden hacer mediante sentencias SQL. En este ejercicio vamos a ver un ejemplo.\n",
    "\n",
    "El objetivo de este ejercicio es contar el número de veces que cada hashtag (palabras precedidas por un #) ha aparecido en el dataset. Para evitar la sobrerrepresentación debida a los retweets vamos a concentrarnos en solo aquellos tweets que no son retweets de ningún otro, o dicho de otra manera, en aquellos en los que el campo ```retweeted_status``` es nulo. Cread una variable ```non_retweets``` que contenga todos estos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Gwon1opoGUG-"
   },
   "outputs": [],
   "source": [
    "non_retweets = tweets.where(\"retweeted_status IS NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avG-xtDqGUG_"
   },
   "source": [
    "Seguidamente vamos a crear una variable ```hashtags``` que contenga una lista de tuplas con la información ```(hashtag, count)```. Para ello, cread un RDD que contenga una lista con el texto de todos los tweets. Una vez hecho este paso tenéis que extraer los hashtags (palabras precedidas por un #) y contarlos.\n",
    "\n",
    "Recordad los conocimientos adquiridos en la PEC 'word count' y el anterior ejercicio, os serán de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A0S68bKCGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#039', 278), ('#laEspañaquequiero', 2), ('#En', 1), ('#checknews', 1), ('#elizabethwarren', 6), ('#amrkplaying', 1), ('#BADAJOZ', 1), ('#ThisistheRealSpain', 1), ('#doodle', 3), ('#VotadInsensatos', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Puede modificarse si se considera conveniente el map y el flatMap\n",
    "import re\n",
    "\n",
    "hashtags = non_retweets.rdd.map(lambda row: row.text) \\\n",
    "                           .flatMap(lambda line: re.findall(r\"#\\w+\", line))\n",
    "\n",
    "hashtags = hashtags.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(hashtags.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciMb5GI4GUG_"
   },
   "source": [
    "Finalmente, se os pide que con el RDD obtenido generéis una tabla ```hashtagsTable``` compuesta de dos columnas:\n",
    "- ***hashtag***\n",
    "- ***num:*** número de veces que aparece cada hashtag.\n",
    "\n",
    "Ordenadla en orden descendente por número de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XV_NgL0oGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             hashtag|   num|\n",
      "+--------------------+------+\n",
      "|                #28A|158133|\n",
      "|   #ElDebateDecisivo|108607|\n",
      "|     #ELDEBATEenRTVE| 94250|\n",
      "|#EleccionesGenera...| 33815|\n",
      "|     #EquiparacionYa| 30567|\n",
      "|       #EleccionesL6| 30075|\n",
      "|         #HazQuePase| 26526|\n",
      "|   #DebateAtresmedia| 21854|\n",
      "|         #DebateRTVE| 17711|\n",
      "|#LaHistoriaLaEscr...| 16964|\n",
      "|          #PorEspaña| 16169|\n",
      "|           #DebatTV3| 14705|\n",
      "|#EleccionesGenerales| 14441|\n",
      "|                #28a| 14327|\n",
      "|         #ILPJusapol| 13488|\n",
      "|            #28Abril| 12848|\n",
      "|         #EspañaViva| 12620|\n",
      "|           #VotaPSOE| 12282|\n",
      "|        #ValorSeguro| 11149|\n",
      "|             #España| 10753|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir RDD a DataFrame\n",
    "hashtagsTable = hashtags.map(lambda row: Row(hashtag=row[0], num=row[1])).toDF().orderBy(\"num\", ascending=False)\n",
    "\n",
    "\n",
    "# Primeras 20 líneas\n",
    "hashtagsTable.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HfsRziG9GUG_"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-80fb386d50cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashtagsTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#28A\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m158124\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output"
     ]
    }
   ],
   "source": [
    "output = hashtagsTable.first()\n",
    "assert output.hashtag == \"#28A\" and output.num == 158124, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "907RhzRjGUG_",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 3:** Sampling\n",
    "\n",
    "En muchas ocasiones, antes de lanzar costoso procesos, es una práctica habitual tratar con un pequeño conjunto de los datos para investigar algunas propiedades o simplemente para debugar nuestros algoritmos, a esta tarea se la llama sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iVwpgsRGUG_"
   },
   "source": [
    "El sampling aplicado es [el homogeneo](https://en.wikipedia.org/wiki/Simple_random_sample). Este sampling se basta en simplemente escoger una fracción de la población seleccionando aleatoriamente elementos de la misma.\n",
    "\n",
    "Primero de todo vamos ha realizar un sampling homogéneo del 1% de los tweets generados en periodo electoral sin reemplazo. Guardad en una variable ```tweets_sample``` este sampling utilizando el método ```sample``` descrito en la [API de pyspark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html). El seed que vais a utilizar para inicializar el generador aleatorio es 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4jPfvfgEGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets sampled: 254364\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "tweets_sample = tweets.sample(False, fraction, seed)\n",
    "\n",
    "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "w-TJslJ6GUHA"
   },
   "outputs": [],
   "source": [
    "assert tweets_sample.count() == 254364, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaQedjXSGUHA"
   },
   "source": [
    "Una de las cosas que resulta interesante comprobar acerca de los patrones de uso de las redes sociales es el patrón de uso diario. En este caso nos interesa el número promedio de tweets que se genera cada hora del día. Para extraer esta información lo que haremos primero, será generar una tabla ```tweets_timestamp``` con la información:\n",
    "- ***created_at***: timestamp de cuando se publicó el tweet.\n",
    "- ***hour***: a que hora del dia corresponde.\n",
    "- ***day***: Fecha en formato MM-dd-YY\n",
    "\n",
    "La fecha que figura en la base de datos esta en la franja horaria GMT. El primer paso es pasar esta información al horario peninsular de España, podéis utilizar la función ```from_utc_timestamp``` para este fin. Así mismo, la función ```hour``` os servirá para extraer la hora del timestamp y la función ```date_format``` os permitirá generar la fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YQKaC5yAGUHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----------------+\n",
      "|         created_at|hour|tweets_timestamp|\n",
      "+-------------------+----+----------------+\n",
      "|2019-04-13 22:57:00|  22|        04-13-19|\n",
      "|2019-04-13 22:57:05|  22|        04-13-19|\n",
      "|2019-04-13 22:57:06|  22|        04-13-19|\n",
      "|2019-04-13 22:57:27|  22|        04-13-19|\n",
      "|2019-04-13 22:57:28|  22|        04-13-19|\n",
      "|2019-04-13 22:57:30|  22|        04-13-19|\n",
      "|2019-04-13 22:57:37|  22|        04-13-19|\n",
      "|2019-04-13 22:57:56|  22|        04-13-19|\n",
      "|2019-04-13 22:58:07|  22|        04-13-19|\n",
      "|2019-04-13 22:58:09|  22|        04-13-19|\n",
      "|2019-04-13 22:58:11|  22|        04-13-19|\n",
      "|2019-04-13 22:58:12|  22|        04-13-19|\n",
      "|2019-04-13 22:58:16|  22|        04-13-19|\n",
      "|2019-04-13 22:58:22|  22|        04-13-19|\n",
      "|2019-04-13 22:58:28|  22|        04-13-19|\n",
      "|2019-04-13 22:58:36|  22|        04-13-19|\n",
      "|2019-04-13 22:58:38|  22|        04-13-19|\n",
      "|2019-04-13 22:58:42|  22|        04-13-19|\n",
      "|2019-04-13 22:58:46|  22|        04-13-19|\n",
      "|2019-04-13 22:58:51|  22|        04-13-19|\n",
      "+-------------------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, hour, from_utc_timestamp\n",
    "\n",
    "tweets_timestamp = tweets_sample.select(\"created_at\")\n",
    "tweets_timestamp = tweets_timestamp.withColumn(\"created_at\", from_utc_timestamp(\"created_at\", \"GMT\"))\n",
    "tweets_timestamp = tweets_timestamp.withColumn(\"hour\", hour(\"created_at\"))\\\n",
    "                                   .withColumn(\"tweets_timestamp\", date_format(\"created_at\", \"MM-dd-YY\"))\n",
    "tweets_timestamp.limit(20).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1SndGF7GUHA"
   },
   "source": [
    "El paso siguiente es agregar estos datos por hora y día en una tabla ```tweets_hour_day```. Tenéis que crear una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** hora del dia\n",
    "- ***day:*** fecha\n",
    "- ***count:*** número de tweets generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6DlTy-B2GUHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----+\n",
      "|hour|tweets_timestamp|count|\n",
      "+----+----------------+-----+\n",
      "|  18|        04-13-19|  525|\n",
      "|  11|        04-17-19|  562|\n",
      "|  10|        04-28-19|  774|\n",
      "|  23|        04-23-19| 4877|\n",
      "|  11|        04-28-19|  993|\n",
      "|   7|        04-22-19|  207|\n",
      "|  16|        04-24-19|  971|\n",
      "|   5|        04-28-19|   78|\n",
      "|   6|        04-29-19|  267|\n",
      "|   4|        04-17-19|   70|\n",
      "|   5|        04-27-19|  108|\n",
      "|  10|        04-14-19|  487|\n",
      "|  23|        04-26-19| 1201|\n",
      "|   7|        04-17-19|  267|\n",
      "|  13|        04-25-19|  734|\n",
      "|  20|        04-28-19| 1587|\n",
      "|   0|        04-23-19| 2808|\n",
      "|   6|        04-23-19|  245|\n",
      "|   7|        04-26-19|  392|\n",
      "|   8|        04-21-19|  269|\n",
      "+----+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_hour_day = tweets_timestamp.groupBy(tweets_timestamp.hour, tweets_timestamp.tweets_timestamp).count()\n",
    "\n",
    "tweets_hour_day.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMhAmufaGUHA"
   },
   "source": [
    "Por último solo nos queda hacer una agregación por hora para conseguir el promedio de tweets por hora. Tenéis que generar una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** Hora\n",
    "- ***tweets:*** Promedio de tweets realizados\n",
    "\n",
    "Recordad que estamos trabajando con un sample del 1% por tanto tenéis que corregir la columna ```tweets``` para que refleje el promedio que deberíamos esperar en el conjunto completo de tweets. La tabla tiene que estar ordenada en orden ascendente de hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0i34Gpq7GUHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|            tweets|\n",
      "+----+------------------+\n",
      "|   0|104822.22222222222|\n",
      "|   1|51744.444444444445|\n",
      "|   2| 23063.15789473684|\n",
      "|   3|12505.555555555555|\n",
      "|   4| 8372.222222222223|\n",
      "|   5|  8264.70588235294|\n",
      "|   6|11977.777777777777|\n",
      "|   7|26455.555555555555|\n",
      "|   8| 40538.88888888889|\n",
      "|   9|54717.647058823524|\n",
      "|  10|           58550.0|\n",
      "|  11| 61761.11111111111|\n",
      "|  12|62266.666666666664|\n",
      "|  13| 66105.55555555555|\n",
      "|  14| 66816.66666666666|\n",
      "|  15| 70644.44444444444|\n",
      "|  16| 69005.55555555555|\n",
      "|  17| 64538.88888888889|\n",
      "|  18| 66616.66666666666|\n",
      "|  19| 67922.22222222222|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "tweets_hour = tweets_hour_day.groupBy(\"hour\").agg(avg(\"count\").alias(\"tweets\")).orderBy(\"hour\").withColumnRenamed(\"avg(tweets_hour_day)\", \"tweets\")\n",
    "             \n",
    "tweets_hour = tweets_hour.withColumn(\"tweets\", tweets_hour.tweets / fraction)\n",
    "tweets_hour.limit(24).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB4vQ-xkGUHC"
   },
   "source": [
    "Por último, tenéis que producir un gráfico de barras utilizando Pandas (pasamos el dataframe a Pandas) donde se muestre la información que acabáis de generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2iw6XjQ2GUHD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8a989a48d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAENCAYAAADzFzkJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHoZJREFUeJzt3X2UVPWd5/H3VwSRCILQEqFZmwhqECcEO0g0GJWITcwJZEVHZowtOpI94sO6OSdinF12E5zgbo6iOz4skY6gRmRMMpIReRDJxMmI0iCCQFw7iNItaksDPq0P4Hf/uL/Wm6Kq+kdVN9UPn9c599S93/v73vur6ur7rftQt8zdERERiXFYqTsgIiIdh4qGiIhEU9EQEZFoKhoiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEO7zUHWhtAwYM8IqKilJ3Q0SkQ1m3bt3b7l7WUrtOVzQqKiqora0tdTdERDoUM3s1pp0OT4mISDQVDRERiaaiISIi0TrdOY1sPvnkE+rr6/nwww9L3ZV2oWfPnpSXl9O9e/dSd0VEOpguUTTq6+vp3bs3FRUVmFmpu1NS7s6uXbuor69n6NChpe6OiHQwLR6eMrMaM3vLzF7MMu+HZuZmNiBMm5ndaWZ1ZrbRzEan2lab2cthqE7FTzOzTSHnTgtbdTM7xsxWhvYrzaxfoU/yww8/pH///l2+YACYGf3799del4gUJOacxv1AVWbQzIYAE4DXUuGJwPAwTAfuCW2PAWYBpwNjgFmpInAPcFUqr3ldM4FV7j4cWBWmC6aC8Tm9FiJSqBaLhrv/AWjKMut24EdA+kfGJwELPbEG6GtmxwHnAyvdvcnddwMrgaowr4+7r/Hkx8oXApNTy1oQxhek4h3Onj17uPvuu9ts+XPnzuWDDz5os+WLiDQr6JyGmU0CGtz9hYxPrYOBHanp+hDLF6/PEgcY6O47w/gbwMBC+ppNxczHW2tRAGyfc0He+c1F4+qrr27V9TabO3cul156Kb169WqT5YtIx5RvW9fSdiuXg77k1sx6AT8G/ltBayxA2AvxXPPNbLqZ1ZpZbWNj46HqVrSZM2fy5z//mVGjRjFt2jSWLFkCwPe+9z2uuOIKAGpqarj55psBePDBBxkzZgyjRo3iBz/4Afv37wdgxYoVfP3rX2f06NFcdNFFvPfee9x55528/vrrnHPOOZxzzjns37+fyy+/nJEjR3Lqqady++23l+ZJi0inVMj3NE4AhgIvmNl2oBxYb2ZfBBqAIam25SGWL16eJQ7wZjh8RXh8K1eH3H2eu1e6e2VZWYu3Tjnk5syZwwknnMCGDRs4//zzefrppwFoaGhgy5YtADz99NOcddZZbN26lUceeYQ//vGPbNiwgW7duvHQQw/x9ttvM3v2bJ588knWr19PZWUlt912G9dddx2DBg1i9erVrF69mg0bNtDQ0MCLL77Ipk2bmDZtWimfuoh0MgddNNx9k7sf6+4V7l5BckhptLu/ASwBLgtXUY0F9oZDTMuBCWbWL5wAnwAsD/PeMbOx4aqpy4DHwqqWAM1XWVWn4h3auHHjePrpp9myZQsjRoxg4MCB7Ny5k2eeeYYzzjiDVatWsW7dOr72ta8xatQoVq1axbZt21izZg1btmzhzDPPZNSoUSxYsIBXXz3wVjFf+tKX2LZtG9deey3Lli2jT58+JXiWItJZtXhOw8weBs4GBphZPTDL3efnaL4U+DZQB3wATANw9yYz+ymwNrT7ibs3n1y/muQKrSOBJ8IAMAdYbGZXAq8CFx/UM2unBg8ezJ49e1i2bBlnnXUWTU1NLF68mKOOOorevXvj7lRXV/Ozn/3sL/J+97vfcd555/Hwww/nXX6/fv144YUXWL58Offeey+LFy+mpqamLZ+SiHQhLRYNd5/awvyK1LgDM3K0qwEO2Hq5ey0wMkt8FzC+pf51BL179+bdd9/9bHrs2LHMnTuXp556il27djFlyhSmTJkCwPjx45k0aRI33HADxx57LE1NTbz77ruMHTuWGTNmUFdXx7Bhw3j//fdpaGjgxBNP/Gz5AwYM4O2336ZHjx5ceOGFnHTSSVx66aWletoi0gl1iW+El1r//v0588wzGTlyJBMnTmTcuHGsWLGCYcOGcfzxx9PU1MS4ceMAGDFiBLNnz2bChAl8+umndO/enbvuuouxY8dy//33M3XqVD766CMAZs+ezYknnsj06dOpqqpi0KBBzJ07l2nTpvHpp58CHLDHIiJSDEt2DjqPyspKz/w9ja1bt/LlL3+5RD1qn/SaiHR+B3PJrZmtc/fKlpapu9yKiEg0FQ0REYmmoiEiItG6TNHobOduiqHXQkQK1SWKRs+ePdm1a5c2lnz+exo9e/YsdVdEpAPqEpfclpeXU19fT3u8L1UpNP9yn4jIweoSRaN79+76lToRkVbQJQ5PiYhI61DREBGRaCoaIiISTUVDRESiqWiIiEg0FQ0REYmmoiEiItFUNEREJJqKhoiIRFPREBGRaCoaIiISrcWiYWY1ZvaWmb2Yiv0vM/uTmW00s9+aWd/UvJvMrM7MXjKz81PxqhCrM7OZqfhQM3s2xB8xsx4hfkSYrgvzK1rrSYuISGFiblh4P/CPwMJUbCVwk7vvM7NbgZuAG81sBHAJcAowCHjSzE4MOXcB5wH1wFozW+LuW4BbgdvdfZGZ3QtcCdwTHne7+zAzuyS0++vinq6ISMdzML/13dZa3NNw9z8ATRmxFe6+L0yuAZrvsz0JWOTuH7n7K0AdMCYMde6+zd0/BhYBk8zMgHOBR0P+AmByalkLwvijwPjQXkRESqQ1zmlcATwRxgcDO1Lz6kMsV7w/sCdVgJrjf7GsMH9vaC8iIiVSVNEws5uBfcBDrdOdgvsx3cxqzaxWP7QkItJ2Ci4aZnY58B3gb/3z31FtAIakmpWHWK74LqCvmR2eEf+LZYX5R4f2B3D3ee5e6e6VZWVlhT4lERFpQUFFw8yqgB8B33X3D1KzlgCXhCufhgLDgeeAtcDwcKVUD5KT5UtCsVkNTAn51cBjqWVVh/EpwFOuH/kWESmpFq+eMrOHgbOBAWZWD8wiuVrqCGBlODe9xt3/k7tvNrPFwBaSw1Yz3H1/WM41wHKgG1Dj7pvDKm4EFpnZbOB5YH6IzwceMLM6khPxl7TC8xURkSK0WDTcfWqW8Pwsseb2twC3ZIkvBZZmiW8juboqM/4hcFFL/RMRkUNH3wgXEZFoKhoiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEU9EQEZFoKhoiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEU9EQEZFoKhoiIhKtxZ977cgqZj6ec972ORccwp6IiHQO2tMQEZFoKhoiIhKtxaJhZjVm9paZvZiKHWNmK83s5fDYL8TNzO40szoz22hmo1M51aH9y2ZWnYqfZmabQs6dZmb51iEiIqUTs6dxP1CVEZsJrHL34cCqMA0wERgehunAPZAUAGAWcDowBpiVKgL3AFel8qpaWIeIiJRIi0XD3f8ANGWEJwELwvgCYHIqvtATa4C+ZnYccD6w0t2b3H03sBKoCvP6uPsad3dgYcaysq1DRERKpNBzGgPdfWcYfwMYGMYHAztS7epDLF+8Pks83zpERKREij4RHvYQvBX6UvA6zGy6mdWaWW1jY2NbdkVEpEsrtGi8GQ4tER7fCvEGYEiqXXmI5YuXZ4nnW8cB3H2eu1e6e2VZWVmBT0lERFpSaNFYAjRfAVUNPJaKXxauohoL7A2HmJYDE8ysXzgBPgFYHua9Y2Zjw1VTl2UsK9s6RESkRFr8RriZPQycDQwws3qSq6DmAIvN7ErgVeDi0Hwp8G2gDvgAmAbg7k1m9lNgbWj3E3dvPrl+NckVWkcCT4SBPOsQEZESabFouPvUHLPGZ2nrwIwcy6kBarLEa4GRWeK7sq1DRERKR98IFxGRaCoaIiISTUVDRESiqWiIiEg0FQ0REYmmoiEiItFUNEREJJqKhoiIRFPREBGRaCoaIiISTUVDRESiqWiIiEg0FQ0REYmmoiEiItFUNEREJJqKhoiIRFPREBGRaCoaIiISTUVDRESiqWiIiEi0w4tJNrMbgL8DHNgETAOOAxYB/YF1wPfd/WMzOwJYCJwG7AL+2t23h+XcBFwJ7Aeuc/flIV4F3AF0A+5z9znF9FdEpJQqZj6ec972ORccwp4UruA9DTMbDFwHVLr7SJIN+yXArcDt7j4M2E1SDAiPu0P89tAOMxsR8k4BqoC7zaybmXUD7gImAiOAqaGtiIiUSLGHpw4HjjSzw4FewE7gXODRMH8BMDmMTwrThPnjzcxCfJG7f+TurwB1wJgw1Ln7Nnf/mGTvZVKR/RURkSIUXDTcvQH4OfAaSbHYS3I4ao+77wvN6oHBYXwwsCPk7gvt+6fjGTm54iIiUiLFHJ7qR/LJfygwCPgCyeGlQ87MpptZrZnVNjY2lqILIiJdQjGHp74FvOLuje7+CfAb4EygbzhcBVAONITxBmAIQJh/NMkJ8c/iGTm54gdw93nuXunulWVlZUU8JRERyaeYovEaMNbMeoVzE+OBLcBqYEpoUw08FsaXhGnC/Kfc3UP8EjM7wsyGAsOB54C1wHAzG2pmPUhOli8por8iIlKkgi+5dfdnzexRYD2wD3gemAc8Diwys9khNj+kzAceMLM6oImkCODum81sMUnB2QfMcPf9AGZ2DbCc5MqsGnffXGh/RUSkeEV9T8PdZwGzMsLbSK58ymz7IXBRjuXcAtySJb4UWFpMH0VEpPXoG+EiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0Yq6ekpEpCvqDHerLZT2NEREJJqKhoiIRFPREBGRaDqnISJ5deXj93Ig7WmIiEg07WmISJvQHkrnpD0NERGJpj0NEemytDd08LSnISIi0bSnIdJFdPZP1Z39+bUXKhoiHYw2jlJKOjwlIiLRtKchIu2K9qTaN+1piIhINBUNERGJVtThKTPrC9wHjAQcuAJ4CXgEqAC2Axe7+24zM+AO4NvAB8Dl7r4+LKca+Puw2NnuviDETwPuB44ElgLXu7sX02eR9kKHYaQjKvacxh3AMnefYmY9gF7Aj4FV7j7HzGYCM4EbgYnA8DCcDtwDnG5mxwCzgEqSwrPOzJa4++7Q5irgWZKiUQU8UWSfRVqVNv7SlRR8eMrMjgbOAuYDuPvH7r4HmAQsCM0WAJPD+CRgoSfWAH3N7DjgfGCluzeFQrESqArz+rj7mrB3sTC1LBERKYFizmkMBRqBX5rZ82Z2n5l9ARjo7jtDmzeAgWF8MLAjlV8fYvni9VniIiJSIsUUjcOB0cA97v5V4H2SQ1GfCXsIbX4Owsymm1mtmdU2Nja29epERLqsYopGPVDv7s+G6UdJisib4dAS4fGtML8BGJLKLw+xfPHyLPEDuPs8d69098qysrIinpKIiORT8Ilwd3/DzHaY2Unu/hIwHtgShmpgTnh8LKQsAa4xs0UkJ8L3uvtOM1sO/IOZ9QvtJgA3uXuTmb1jZmNJToRfBvzvQvsr0hKd0BZpWbFXT10LPBSunNoGTCPZe1lsZlcCrwIXh7ZLSS63rSO55HYaQCgOPwXWhnY/cfemMH41n19y+wS6ckpEpKSKKhruvoHkUtlM47O0dWBGjuXUADVZ4rUk3wEREZF2QN8IFxGRaCoaIiISTXe5lU5HJ7RF2o72NEREJJqKhoiIRFPREBGRaCoaIiISTSfCs9CJVBGR7LSnISIi0VQ0REQkmoqGiIhEU9EQEZFoKhoiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEU9EQEZFoRRcNM+tmZs+b2b+E6aFm9qyZ1ZnZI2bWI8SPCNN1YX5Fahk3hfhLZnZ+Kl4VYnVmNrPYvoqISHFaY0/jemBravpW4HZ3HwbsBq4M8SuB3SF+e2iHmY0ALgFOAaqAu0Mh6gbcBUwERgBTQ1sRESmRooqGmZUDFwD3hWkDzgUeDU0WAJPD+KQwTZg/PrSfBCxy94/c/RWgDhgThjp33+buHwOLQlsRESmRYn9PYy7wI6B3mO4P7HH3fWG6HhgcxgcDOwDcfZ+Z7Q3tBwNrUstM5+zIiJ9eZH+lA9Hvmoi0PwXvaZjZd4C33H1dK/an0L5MN7NaM6ttbGwsdXdERDqtYg5PnQl818y2kxw6Ohe4A+hrZs17MOVAQxhvAIYAhPlHA7vS8YycXPEDuPs8d69098qysrIinpKIiORTcNFw95vcvdzdK0hOZD/l7n8LrAamhGbVwGNhfEmYJsx/yt09xC8JV1cNBYYDzwFrgeHhaqweYR1LCu2viIgUry1+I/xGYJGZzQaeB+aH+HzgATOrA5pIigDuvtnMFgNbgH3ADHffD2Bm1wDLgW5AjbtvboP+iohIpFYpGu7+e+D3YXwbyZVPmW0+BC7KkX8LcEuW+FJgaWv0UUREiqdvhIuISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEU9EQEZFoKhoiIhJNRUNERKKpaIiISDQVDRERiaaiISIi0VQ0REQkmoqGiIhEU9EQEZFoKhoiIhKtLX6ESeQvVMx8POe87XMuOIQ9EZFiaU9DRESiqWiIiEg0FQ0REYmmoiEiItF0IrwV6YSviHR2Be9pmNkQM1ttZlvMbLOZXR/ix5jZSjN7OTz2C3EzszvNrM7MNprZ6NSyqkP7l82sOhU/zcw2hZw7zcyKebIiIlKcYg5P7QN+6O4jgLHADDMbAcwEVrn7cGBVmAaYCAwPw3TgHkiKDDALOB0YA8xqLjShzVWpvKoi+isiIkUquGi4+053Xx/G3wW2AoOBScCC0GwBMDmMTwIWemIN0NfMjgPOB1a6e5O77wZWAlVhXh93X+PuDixMLUtEREqgVU6Em1kF8FXgWWCgu+8Ms94ABobxwcCOVFp9iOWL12eJi4hIiRRdNMzsKODXwH9293fS88Ieghe7jog+TDezWjOrbWxsbOvViYh0WUUVDTPrTlIwHnL334Twm+HQEuHxrRBvAIak0stDLF+8PEv8AO4+z90r3b2yrKysmKckIiJ5FHP1lAHzga3ufltq1hKg+QqoauCxVPyycBXVWGBvOIy1HJhgZv3CCfAJwPIw7x0zGxvWdVlqWSIiUgLFfE/jTOD7wCYz2xBiPwbmAIvN7ErgVeDiMG8p8G2gDvgAmAbg7k1m9lNgbWj3E3dvCuNXA/cDRwJPhEFEREqk4KLh7v8G5PrexPgs7R2YkWNZNUBNlngtMLLQPoqISOvSbURERCSaioaIiERT0RARkWgqGiIiEk1FQ0REoqloiIhINP2eRjvQUX6Ho6P0U0TajvY0REQkmoqGiIhEU9EQEZFoKhoiIhJNRUNERKLp6qkOrNCrmXQVlIgUSnsaIiISTUVDRESiqWiIiEg0FQ0REYmmoiEiItFUNEREJJqKhoiIRGv3RcPMqszsJTOrM7OZpe6PiEhX1q6Lhpl1A+4CJgIjgKlmNqK0vRIR6braddEAxgB17r7N3T8GFgGTStwnEZEuq70XjcHAjtR0fYiJiEgJmLuXug85mdkUoMrd/y5Mfx843d2vyWg3HZgeJk8CXsqxyAHA2wV0RXkdN68j9FF5ymsPece7e1mLS3D3djsAXweWp6ZvAm4qYnm1yutaeR2hj8pTXnvPSw/t/fDUWmC4mQ01sx7AJcCSEvdJRKTLate3Rnf3fWZ2DbAc6AbUuPvmEndLRKTLatdFA8DdlwJLW2lx85TX5fI6Qh+Vp7z2nveZdn0iXERE2pf2fk5DRETaERUNERGJ1u7PaRTDzE4m+QZ58xcCG4Al7r61dL06kJmNAdzd14bbpFQBfwrnc2KXsdDdL2uzTh5CqSvlXnf3J83sb4AzgK3APHf/pKQdFOnCOu05DTO7EZhKcuuR+hAuJ9kYLXL3OW2wzpNJCtSz7v5eKl7l7sty5MwiubfW4cBK4HRgNXAeyXdUbsmSk3nZsQHnAE8BuPt3I/v7DZJbtbzo7ivytDsd2Oru75jZkcBMYDSwBfgHd9+bI+864LfuviPb/Dzre4jk9egF7AGOAn4DjCd5z1bnyf0S8B+BIcB+4P8Cv3L3dw6mDyKSQ7Ff9GivA8nGonuWeA/g5QKXOS3PvOtIvon+z8B2YFJq3vo8eZtILifuBbwD9AnxI4GNOXLWAw8CZwPfDI87w/g386zrudT4VcAGYBbwR2BmnrzNwOFhfB4wF/hGyP1Nnry9wOvA08DVQFnk67wxPB4OvAl0C9OW6zVJ/Q1WAH8P/DvJzS5vISluZ5f6PdmeBuDYQ7y+/qV+zq34XI4G5gB/ApqAXSR7wXOAvgUu84k88/oAPwMeAP4mY97defK+CNwT/g/6A/89bG8WA8cV/PxL/Qdowz/sn0i+Fp8ZPx54qcBlvpZn3ibgqDBeAdQC14fp5/PkPZ9tPExvyJFzGHADyZ7JqBDbFtH/9LrWNm/EgS8Am/LkbU2Nr4/pY/P6Ql8nAPOBRmAZUA30zpP3Iklx7we8CxwT4j3TfcnxN2guML2A34fx/5DvbxDatOqGoD1tBIBjMob+JB9s+jW/tjnyqjJen/nARuBXwMA8eXOAAWG8EtgG1AGvkv9DzXqSgn/CQb7WlSR75w+S7GGuJPnAshb4ap68o4CfkHwo2hven2uAy1tY33LgRuCLGX+bG4EVefJG5xhOA3bmyft1eE0nk3y5+dfAEdn+HzPylgHXkhwZ2Bj6NyTEHjvY9/Rnyy00sb0PJOcF6oAnSD4dzwsvYl36nyFL3sYcwybgozx5m7O8IZcBt5F/w/os0CuMH5aKH53vDRHalAP/BPwjeQpaqv0LYUPRn4zbCZC/sP0TYS8L+CVQGcZPBNbmycssMN2B7wIPA4158m4IG5pXSfYeVgG/CH+DWXnyNqX+mfqlnyPJIbh8r81Bbwg6ykYA+BR4JWP4JDzm/LCR7gtwHzCb5EPXDcA/5/s7pMZXA19LvV9y3sYi9OfnwGvAc2E9gyLe18+RHOKdSnKD0ykhPh54Jk/eY8Dl4f/ovwD/FRgOLCA57JorL+eHzhbm7Sc5hLw6y/D/8uRtyJi+meToQP8W3i/pD4mv5VvmwQwFJXWUgeRT7ljgwjCMJXwSzZPzJjAq/HOkhwqSE7O58p4ifOpPxQ4HFgL78+QdkSM+ADg18nlekO9Nnmq3nWRj/Ep4PC7Ej8r3JiIpYPcDfyYpcp+E/H8FvpInL18h6tVCXwc1bzCAvsAUYEwLOdeTbEx/QbLH0FzoyoA/tJB70BuCjrIRAH5IUnBOTcVeiXi/rM+1/BbWt5XPD2euyZiXb482vb5xwN3AG+H1nF7g65LvPfhCxvTa8HgYyYUoufJWAD8itbcFDCQp4k/myXsRGJ5j3o4WXs/DMmKXk+whvRrz/IDZsX+HFt8XhSZ21oFkF/wbOeb9Kk9eOalPqRnzziz182rhOfcChka06wN8heSTdM7DE6n2J5bguZwSCszJB5l30BuCjrQR4PO90tuA3sQdzqwn+QT+Q5IPCZaal+/c0rXh9TyX5BDaHSTn2/4H8ECevAMKJsn5virgl3nyniE5BHoRyd7p5BD/Jvn3bP69+X+dZA84fXPUfB8i+gG3knww2U1yOHNriOU73DcFOCnHvMl58v4n8K0s8SrynJ8lOfR2VJb4MODRmP+LrMstNFGDhs40ZGwImjI2BP1y5HS4jUDYOK4B3ohoOytjaD4H9kVgYQu5ZwOPkJzX2kRyK6DphD2QHDmLCvzbfYXk8OITwMmhSO0hKcJn5Mn7K5JDW7uBfyN8yCHZM72uhXWeDHwr8+9BnkPfqbzxrZg3sS3Wl3eZhSZq0NBVBvJcNdeaOYcqj+TKvJHtvZ/tNY/Cr5QsNO/aQ5nX4mtTaKIGDV1lIOIig9bIUV7HyKPwKyU7RF5LQ6f+RrhILDPbmGsWybmNVslRXsfPIzkf9R6Au283s7OBR83s+JDb0fPyUtEQSQwEzic5vp1mJCdMWytHeR0/700zG+XuGwDc/T0z+w5QA5zaCfLyUtEQSfwLya78hswZZvb7VsxRXsfPuwzYlw64+z7gMjP7P50gL69Oe+8pERFpfbo1uoiIRFPREBGRaCoaIkUyswoze7HU/RA5FFQ0RNohM9NFKtIuqWiItI5uZvYLM9tsZivM7EgzG2Vma8xso5n91sz6QXJljplVhvEBZrY9jF9uZkvM7CmSO/uKtDsqGiKtYzhwl7ufQnLfowtJ7nB8o7v/FeG27hHLGU1ya+9vtllPRYqgoiHSOl5JXe+/DjiB5Meb/jXEFgBnRSxnpbs3tUUHRVqDioZI6/goNb6f5DdActnH5/97PTPmvd+anRJpbSoaIm1jL7DbzMaF6e+T/GgVJHccPS2MTznE/RIpiq7QEGk71cC9ZtaL5EeMpoX4z4HFZjYdeLxUnRMphG4jIiIi0XR4SkREoqloiIhINBUNERGJpqIhIiLRVDRERCSaioaIiERT0RARkWgqGiIiEu3/AxJiLDOy0yDGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_hour_pd = tweets_hour.toPandas()\n",
    "tweets_hour_pd.plot.bar(x=\"hour\", y=\"tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-VtaqK2GUHD",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 4:** Introducción a los datos relacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6v0z-w0GUHD"
   },
   "source": [
    "El hecho de trabajar con una base de datos que contiene información generada en una red social nos permite introducir el concepto de datos relacionales. Podemos definir datos relacionales como aquellos en los que existen relaciones entre las entidades que constituyen la base de datos. Si estas relaciones son binarias, relaciones 1 a 1, podemos representar las relaciones como un grafo compuesto por un conjunto de vértices $\\mathcal{V}$ y un conjunto de aristas $\\mathcal{E}$ que los relacionan.\n",
    "\n",
    "En el caso de grafos que emergen de manera orgánica, este tipo de estructura va más allá de los grafos regulares que seguramente conocéis. Este tipo de estructuras se conocen como [redes complejas](https://es.wikipedia.org/wiki/Red_compleja). El estudio de la estructura y dinámicas de este tipo de redes ha contribuido a importantes resultados en campos tan dispares como la física, la sociología, la ecología o la medicina.\n",
    "\n",
    "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
    "\n",
    "En esta última parte de la práctica vamos a trabajar con este tipo de datos. En concreto vamos a modelar uno de los posibles relaciones presentes en el dataset, la red de retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj3QBdQXGUHD"
   },
   "source": [
    "### Generar la red de retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inpex3H9GUHD"
   },
   "source": [
    "#### **Parte 4.1**: Construcción de la edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOA8W2sGUHD"
   },
   "source": [
    "Lo primero se os pide es que generéis la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teórico, la manera más habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta práctica vamos a centrarnos en el aspecto computacional, una de las maneras de mas eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relación a parejas entre las entidades.\n",
    "\n",
    "Las relaciones pueden ser bidireccionales o direccionales y tener algún peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuántas veces esto ha pasado.\n",
    "\n",
    "Lo primero que haréis para simplificar el cómputo,  es crear un sample homogéneo sin reemplazo del 1% de los tweets. Utilizando los conocimientos que habéis aprendido en el apartado 3.1. Utilizaremos 42 como valor para la seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Vxahz1fJGUHD"
   },
   "outputs": [],
   "source": [
    "sample = tweets.sample(False, 0.01, 42).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwojQwBIGUHD"
   },
   "source": [
    "Ahora vais a crear una tabla ```edgelist``` con la siguiente información:\n",
    "- ***src:*** usuario que retuitea\n",
    "- ***dst:*** usuario que es retuiteado\n",
    "- ***weight:*** número de veces que un usuario retuitea a otro.\n",
    "\n",
    "Filtrar el resultado para que contenga sólo las relaciones con un weight igual o mayor a dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wImuemZDGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5243 edges on the network.\n"
     ]
    }
   ],
   "source": [
    "edgelist = sample.where(\"retweeted_status IS NOT NULL\")\\\n",
    "                 .selectExpr(\"user.screen_name AS src\", \"retweeted_status.user.screen_name AS dst\")\n",
    "\n",
    "edgelist = edgelist.groupBy(\"src\", \"dst\").count().withColumnRenamed(\"count\", \"weight\").where(\"weight >= 2\")\n",
    "\n",
    "L = edgelist.count()\n",
    "\n",
    "print(\"There are {0} edges on the network.\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bbkDixqiGUHE"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect ouput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-94f5e6f4b351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5247\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect ouput\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect ouput"
     ]
    }
   ],
   "source": [
    "assert L == 5247, \"Incorrect ouput\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCehZGLWGUHE"
   },
   "source": [
    "#### **Parte 4.2:** Centralidad de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XvkaBHFGUHE"
   },
   "source": [
    "Uno de los descriptores más comunes en el análisis de redes es el grado. El grado cuantifica cuántas aristas están conectadas a cada vértices. En el caso de redes dirigidas como la que acabamos de crear este descriptor está descompuesto en el:\n",
    "- **in degree**: cuantas aristas apuntan al nodo\n",
    "- **out degree**: cuantas aristas salen del nodo\n",
    "\n",
    "Si haces un ranquing de estos valores vais a obtener medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
    "\n",
    "Se os pide que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario (anteriormene src).\n",
    "- ***outDegree:*** out degree del nodo.\n",
    "\n",
    "Ordenado la tabla por out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tTfZfEOGGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|            src|outDegree|\n",
      "+---------------+---------+\n",
      "|   rosavergar23|       12|\n",
      "|     carrasquem|       10|\n",
      "|Teresaperezcep1|       10|\n",
      "|  joanagabarrof|        9|\n",
      "|MarcoCostaValue|        9|\n",
      "|     pallaron12|        9|\n",
      "|   Socialista60|        8|\n",
      "|    SSarelvis67|        8|\n",
      "|JulioAl18175505|        8|\n",
      "|       jasalo54|        8|\n",
      "|MariaJo40891027|        8|\n",
      "|   Pepa63306772|        7|\n",
      "|    RosaMar6254|        7|\n",
      "|     Nacho_JISF|        7|\n",
      "|     nusagatero|        7|\n",
      "|      ISMVALSyR|        7|\n",
      "|    mariaje1956|        7|\n",
      "|      el_partal|        7|\n",
      "|        lyuva26|        7|\n",
      "|  PabloChabolas|        6|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "outDegree = edgelist.groupBy(\"src\").agg({\"dst\": \"count\"}).withColumnRenamed(\"count(dst)\", \"outDegree\").orderBy(\"outDegree\", ascending=False)\n",
    "\n",
    "outDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "6jvwDBaPGUHE"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "screen_name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0;31m# but this will not be used in normal cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'screen_name' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6bf158fb7e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutDegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rosavergar23\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutDegree\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: screen_name"
     ]
    }
   ],
   "source": [
    "output = outDegree.first()\n",
    "assert output.screen_name == \"rosavergar23\" and output.outDegree == 11, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAWf7I9CGUHE"
   },
   "source": [
    "Se os pide ahora que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario (anteriormente dst).\n",
    "- ***inDegree:*** in degree del nodo.\n",
    "\n",
    "Ordenad la tabla por in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "erRzYC6MGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|            dst|inDegree|\n",
      "+---------------+--------+\n",
      "|         vox_es|     321|\n",
      "|   ahorapodemos|     275|\n",
      "|           PSOE|     256|\n",
      "|   CiudadanosCs|     202|\n",
      "|  Santi_ABASCAL|     200|\n",
      "|      populares|     133|\n",
      "|  AlbanoDante76|     105|\n",
      "|           KRLS|      93|\n",
      "|Front_Republica|      90|\n",
      "|Pablo_Iglesias_|      80|\n",
      "|       ivanedlm|      70|\n",
      "|       iescolar|      70|\n",
      "|sanchezcastejon|      69|\n",
      "|         boye_g|      57|\n",
      "| AntonioMaestre|      56|\n",
      "|      JuntsXCat|      56|\n",
      "| hermanntertsch|      55|\n",
      "|     eldiarioes|      46|\n",
      "|   pablocasado_|      40|\n",
      "|    CastigadorY|      39|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replicando el código utilizado para generar el outDegree, generad el inDegree\n",
    "inDegree = edgelist.groupBy(\"dst\").agg({\"src\": \"count\"}).withColumnRenamed(\"count(src)\", \"inDegree\").orderBy(\"inDegree\", ascending=False)\n",
    "\n",
    "inDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQzI17PJGUHF"
   },
   "outputs": [],
   "source": [
    "output = inDegree.first()\n",
    "assert output.screen_name == \"vox_es\" and output.inDegree == 330, \"Incorrect output\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S4Co-KvEGUG5",
    "SyzhlfUFGUG-",
    "8hTeGFtnGUG-",
    "Inpex3H9GUHD",
    "zCehZGLWGUHE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
