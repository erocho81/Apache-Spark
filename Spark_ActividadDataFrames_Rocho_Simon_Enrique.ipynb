{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmh-0M-EGUGt"
   },
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# PEC 2: Parte 2, Marzo 2023\n",
    "\n",
    "## ExtracciÃ³n de conocimiento de fuentes de datos heterogÃ©neas mediante Spark SQL, RDDs y GraphFrames\n",
    "\n",
    "En esta prÃ¡ctica vamos a introducir estructuras de datos mÃ¡s complejas que las vistas hasta ahora, donde los campos pueden a su vez tener campos anidados. En concreto utilizaremos datos de twitter capturados en el contexto de las elecciones generales en EspaÃ±a del 28 de Abril de 2019. La prÃ¡ctica estÃ¡ estructurada de la siguiente manera:\n",
    "- **Parte 0:** ConfiguraciÃ³n del entorno\n",
    "- **Parte 1:** IntroducciÃ³n a data frames estructurados y cÃ³mo operar extraer informaciÃ³n *(3 puntos)*\n",
    "    - **Parte 1.1:** Importar los datos *(1 puntos)*\n",
    "    - **Parte 1.2:** *Queries* sobre sobre data frames complejos *(2 puntos)*\n",
    "        - **Parte 1.2.1:** Queries SQL *(1 puntos)*\n",
    "        - **Parte 1.2.2:** Queries sobre el pipeline *(1 puntos)*\n",
    "- **Parte 2:** Bases de datos HIVE y operaciones complejas *(3 puntos)*\n",
    "    - **Parte 2.1:** Bases de datos Hive *(1 puntos)*\n",
    "    - **Parte 2.2:** MÃ¡s allÃ¡ de las transformaciones SQL *(2 puntos)*\n",
    "        - **Parte 2.2.1:** Tweets por poblaciÃ³n  *(1 puntos)*\n",
    "        - **Parte 2.2.2:** Contar hashtags *(1 puntos)*\n",
    "- **Parte 3:** Sampling *(2 Puntos)*\n",
    "- **Parte 4**: IntroducciÃ³n a los datos relacionales *(2 puntos)*\n",
    "     - **Parte 4.1**: ConstrucciÃ³n de la edgelist *(1 puntos)*\n",
    "     - **Parte 4.2**: Centralidad de grado *(1 puntos)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ASnskRaGUGx"
   },
   "source": [
    "## **Parte 0:** ConfiguraciÃ³n del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import random\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master=\"local[1]\", appName=\"PEC3_erocho\")\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2qE1sQRGUG0",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 1:** IntroducciÃ³n a data frames estructurados y operaciones sobre ellos.\n",
    "\n",
    "Como ya se ha mencionado, en esta prÃ¡ctica vamos a utilizar datos de Twitter que recolectamos durante las elecciones generales en EspaÃ±a del 28 de abril de 2019. Como veremos, los tweets tienen una estructura interna bastante compleja que hemos simplificado un poco en esta prÃ¡ctica.\n",
    "\n",
    "### **Parte 1.1:** Importar los datos\n",
    "\n",
    "Lo primero que vamos ha aprender es cÃ³mo importar este tipo de datos a nuestro entorno. Uno de los tipos de archivos mÃ¡s comunes para guardar este formato de informaciÃ³n es [la estructura JSON](https://en.wikipedia.org/wiki/JSON). Esta estructura permite guardar informaciÃ³n en un texto plano de diferentes objetos siguiendo una estructura de diccionario donde cada campo tiene asignado una llave y un valor. La estructura puede ser anidada, o sea que una llave puede tener como valor otra estructura tipo diccionario.\n",
    "\n",
    "Spark SQL permite leer datos de muchos formatos diferentes (como recordareis de la anterior prÃ¡ctica donde leÃ­mos un fichero CSV). En esta ocasiÃ³n, se os pide que leÃ¡is un fichero JSON de la ruta ```/aula_22.419/data/tweets28a_sample.json```. Este archivo contiene un pequeÃ±o *sample*, un 0.1% de la base de datos completa (en un siguiente apartado veremos cÃ³mo realizar este *sampleado*). En esta ocasiÃ³n no se os pide especificar la estructura del data frame ya que la funciÃ³n de lectura la inferirÃ¡ automÃ¡ticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3R3EQBVYGUG0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 27268 tweets\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "tweets_sample = sqlContext.read.json(\"/aula_22.419/data/tweets28a_sample.json\")\n",
    "\n",
    "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj93OIWeGUG1"
   },
   "source": [
    "El siguiente paso es mostrar la estructura del dataset que acabamos de cargar. Recordad que podÃ©is obtener la informaciÃ³n acerca de cÃ³mo estÃ¡ estructurado el DataTable utilizando el mÃ©todo ```printSchema()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9sL3masaGUG1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv0DNkyYGUG2"
   },
   "source": [
    "PodÃ©is observar que la estructura del tweet contiene mÃºltiples campos anidados. Teneis que familiarizaros con esta estructura ya que serÃ¡ la que utilizaremos durante toda la prÃ¡ctica. Recordad tambiÃ©n que no todos los tweets tienen todos los campos, como por ejemplo la ubicaciÃ³n (campo ```place```). Cuando esto pasa el campo pasa a ser ```NULL```. PodÃ©is ver mas informaciÃ³n sobre este tipo de datos en [este enlace](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tvG-wtZGUG2",
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 1.2:** *Queries* sobre sobre data frames complejos\n",
    "\n",
    "En esta parte vamos a introducir conceptos sobre cÃ³mo trabajar con data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfQmHucCGUG2"
   },
   "source": [
    "#### **Parte 1.2.1:** Queries SQL\n",
    "\n",
    "El primer paso consiste en registrar la tabla en el contexto SQL comprobando primero si existe y borrÃ¡ndola en el caso que sea asÃ­. En este apartado se os pide que registrÃ©is la tabla ```tweets_sample``` que acabamos de cargar en el contexto sql bajo el mismo nombre ```tweets_sample```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eld-cll_GUG2"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: tweets_sample; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: tweets_sample; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:651)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'tweets_sample' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:84)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:120)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:746)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:146)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 73 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4cf1d7a4fc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Primero comprobamos si hay algo para la tabla tweets_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from tweets_sample LIMIT 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: tweets_sample; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "#Primero comprobamos si hay algo para la tabla tweets_sample\n",
    "sqlContext.sql(\"select * from tweets_sample LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registramos\n",
    "sqlContext.registerDataFrameAsTable(tweets_sample, \"tweets_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='1122569971345833984', created_at=1556476512, lang='und', place=None, retweeted_status=Row(_id='1122566927866634245', user=Row(followers_count=3109, friends_count=2978, id_str='301428215', lang='es', screen_name='Ibero_DMJ', statuses_count=52713)), text='RT @Ibero_DMJ: ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ https://t.co/GkuSJPpboj', user=Row(followers_count=2511, friends_count=4383, id_str='1069389675645870080', lang='en', screen_name='jhlacasa1', statuses_count=3360)),\n",
       " Row(_id='1122570131736006656', created_at=1556476550, lang='pt', place=None, retweeted_status=None, text='@CervantesFAQs @Cazatalentos @vox_es @Irene_Montero_ Asco de gente y de VðŸ’©X!', user=Row(followers_count=33, friends_count=119, id_str='1030176768056012801', lang='es', screen_name='TeresaAngelin15', statuses_count=1838)),\n",
       " Row(_id='1122570215445999617', created_at=1556476570, lang='es', place=None, retweeted_status=Row(_id='1122561071426998273', user=Row(followers_count=1101665, friends_count=415, id_str='26729931', lang='es', screen_name='rtve', statuses_count=198717)), text='RT @rtve: El PSOE ganarÃ­a las elecciones y necesitarÃ­a a los independentistas para gobernar, segÃºn el sondeo de Gad3 para RTVE  https://t.câ€¦', user=Row(followers_count=1741, friends_count=1498, id_str='250333298', lang='es', screen_name='LidiaSierraBUR', statuses_count=14759)),\n",
       " Row(_id='1122570274610851846', created_at=1556476584, lang='es', place=None, retweeted_status=None, text='#SiguemeYTeSigoVox voy a intentar volver a tener una cuenta de polÃ­tica aunque la izquierda me haya censurado ya vaâ€¦ https://t.co/5ujCE1bgPt', user=Row(followers_count=71, friends_count=243, id_str='1079050390472216576', lang='es', screen_name='DanielJ84562737', statuses_count=565)),\n",
       " Row(_id='1122570283129479169', created_at=1556476586, lang='es', place=None, retweeted_status=Row(_id='1122541491895832577', user=Row(followers_count=493, friends_count=695, id_str='193802591', lang='es', screen_name='Aljiuss', statuses_count=14700)), text='RT @Aljiuss: AhÃ­ lo teneis, Rafael MartÃ­nez 105 aÃ±os. En 1936 votÃ³ al Frente Popular y en 2019 votando a Unidas Podemos.... Ahora todas y tâ€¦', user=Row(followers_count=1, friends_count=31, id_str='1122490258791260160', lang='en', screen_name='Elen29842466', statuses_count=2)),\n",
       " Row(_id='1122570300904878080', created_at=1556476590, lang='es', place=Row(bounding_box=Row(coordinates=[[[-5.274768, 37.080994], [-5.274768, 37.39277], [-4.965451, 37.39277], [-4.965451, 37.080994]]], type='Polygon'), country_code='ES', id='f33076431b70b3a3', name='Osuna', place_type='city'), retweeted_status=None, text='Ejemplos de porque la prensa pierde credibilidad a diario', user=Row(followers_count=1130, friends_count=1029, id_str='555078632', lang='es', screen_name='JUANDEDIOSFISIO', statuses_count=6379)),\n",
       " Row(_id='1122570301940871168', created_at=1556476590, lang='pt', place=None, retweeted_status=Row(_id='1122300505810714625', user=Row(followers_count=106964, friends_count=972, id_str='43340387', lang='en', screen_name='DCM_online', statuses_count=100332)), text='RT @DCM_online: Pesquisa Vox Populi: Lula 10 x Bolsonaro 1 https://t.co/su98bVMNu4', user=Row(followers_count=177, friends_count=372, id_str='77369029', lang='pt', screen_name='SpinaOficial', statuses_count=30608)),\n",
       " Row(_id='1122570422178938880', created_at=1556476619, lang='und', place=None, retweeted_status=None, text='ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚', user=Row(followers_count=5808, friends_count=507, id_str='139452633', lang='es', screen_name='sergiofmarca', statuses_count=25654)),\n",
       " Row(_id='1122570463962595334', created_at=1556476629, lang='es', place=None, retweeted_status=Row(_id='1122570189760028673', user=Row(followers_count=205, friends_count=187, id_str='1055192007986028546', lang='es', screen_name='Oktubrenoticias', statuses_count=841)), text='RT @Oktubrenoticias: Las encuestas de GAD3 y COPE dan al PSOE la posibilidad de gobernar con el apoyo de Unidas Podemos e independentistasâ€¦', user=Row(followers_count=99, friends_count=68, id_str='1057047241859444736', lang='es', screen_name='eligiendouy', statuses_count=37)),\n",
       " Row(_id='1122570610381660160', created_at=1556476664, lang='es', place=None, retweeted_status=Row(_id='1122523272300548096', user=Row(followers_count=17605, friends_count=641, id_str='337731568', lang='es', screen_name='_iMperfectB', statuses_count=46610)), text='RT @_iMperfectB: Los que tenÃ©is familiares que votan a VOX y soportÃ¡is sus comentarios e ideales de mierda prÃ¡cticamente a diario porque noâ€¦', user=Row(followers_count=543, friends_count=426, id_str='1557641346', lang='es', screen_name='xlbxmxnz', statuses_count=20388))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisamos ahora si hay info de nuevo:\n",
    "sqlContext.sql(\"select * from tweets_sample LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjbyKTvOGUG3"
   },
   "source": [
    "Ahora se os pide que creeis una tabla ```users_agg``` con [la informaciÃ³n agregada](https://www.w3schools.com/sql/sql_groupby.asp) de los usuarios que tengan definido su idioma (```user.lang```) como espaÃ±ol (```es```). En concreto se os pide que la tabla contenga las siguientes columnas:\n",
    "- **screen_name:** nombre del usuario\n",
    "- **friends_count:** nÃºmero mÃ¡ximo (ver nota) de personas a las que sigue\n",
    "- **tweets:** nÃºmero de tweets realizados\n",
    "- **followers_count:** nÃºmero mÃ¡ximo (ver nota) personas que siguen al usuario.\n",
    "\n",
    "El orden en el cual se deben mostrar los registros es orden descendente acorde al nÃºmero de tweets.\n",
    "\n",
    "***Nota:*** es importante que os fijÃ©is que el nombre de *friends* i *followers* puede diferir a lo largo de la adquisiciÃ³n de datos. En este caso vamos ha utilizar la funciÃ³n de agregaciÃ³n ```MAX``` sobre cada uno de estos campos para evitar segmentar el usuario en diversas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M0MmT_DBGUG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  PabloChabolas|         4925|     9|           4042|\n",
      "|      lolalailo|         4922|     9|           3738|\n",
      "|     Lordcrow11|         5002|     9|           3069|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg = sqlContext.sql(\"\"\"  SELECT user.screen_name, MAX(user.friends_count) AS friends_count, COUNT (_id) as tweets, MAX(user.followers_count) AS followers_count\n",
    "                                FROM tweets_sample\n",
    "                                WHERE user.lang == 'es'\n",
    "                                GROUP BY user.screen_name\n",
    "                                ORDER BY tweets DESC\"\"\")\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "j-ECTvkjGUG3"
   },
   "outputs": [],
   "source": [
    "output = users_agg.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "551-Szm6GUG4"
   },
   "source": [
    "Imaginad ahora que queremos combinar la informaciÃ³n que acabamos de generar con informaciÃ³n acerca del nÃºmero de veces que un usuario ha sido retuiteado. Para hacer este tipo de combinaciones necesitamos recurrir al [```JOIN``` de tablas](https://www.w3schools.com/sql/sql_join.asp). Primero debemos registrar la tabla que acabamos de generar en el contexto SQL. Recordad que primero debÃ©is comprobar si la tabla existe y en caso afirmativo eliminarla ('drop table if exists <tabla>'). La tabla tenÃ©is que registrarla bajo el nombre de ```user_agg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RUrrffcJGUG4"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: user_agg; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: user_agg; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:651)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'user_agg' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:84)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:120)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:747)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:746)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:146)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 73 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dfb9f96b489e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from user_agg LIMIT 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: user_agg; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "#Revisamos si existe:\n",
    "sqlContext.sql(\"select * from user_agg LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registramos:\n",
    "sqlContext.registerDataFrameAsTable(users_agg,\"user_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(screen_name='anaoromi', friends_count=6258, tweets=16, followers_count=6774),\n",
       " Row(screen_name='RosaMar6254', friends_count=6208, tweets=14, followers_count=6245),\n",
       " Row(screen_name='lyuva26', friends_count=3088, tweets=13, followers_count=3732),\n",
       " Row(screen_name='PisandoFuerte10', friends_count=2795, tweets=12, followers_count=1752),\n",
       " Row(screen_name='carrasquem', friends_count=147, tweets=12, followers_count=215),\n",
       " Row(screen_name='jasalo54', friends_count=1889, tweets=11, followers_count=689),\n",
       " Row(screen_name='lolalailo', friends_count=4922, tweets=9, followers_count=3738),\n",
       " Row(screen_name='Lordcrow11', friends_count=5002, tweets=9, followers_count=3069),\n",
       " Row(screen_name='DuroBelinda', friends_count=5242, tweets=9, followers_count=5778),\n",
       " Row(screen_name='PabloChabolas', friends_count=4925, tweets=9, followers_count=4042)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisamos de nuevo\n",
    "sqlContext.sql(\"select * from user_agg LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwLsGWbeGUG4"
   },
   "source": [
    "Una vez registrada se pide que combinÃ©is esta tabla y la tabla ```tweets_sample``` utilizando un ```INNER JOIN``` para obtener una nueva tabla con la siguiente informaciÃ³n:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***friends_count:*** nÃºmero mÃ¡ximo de personas a las que sigue\n",
    "- ***followers_count:*** nÃºmero mÃ¡ximo de personas que siguen al usuario.\n",
    "- ***tweets:*** nÃºmero de tweets realizados por el usuario.\n",
    "- ***retweeted:*** nÃºmero de retweets obtenidos por el usuario.\n",
    "- ***ratio_tweet_retweeted:*** ratio de retweets por nÃºmero de tweets publicados $\\frac{retweets}{tweets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I4RPdH8qGUG4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|   screen_name|friends_count|followers_count|tweets|retweeted|ratio_tweet_retweeted|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|          PSOE|        13635|         671073|     1|      155|                155.0|\n",
      "|  CiudadanosCs|        92910|         511896|     1|      117|                117.0|\n",
      "|     JuntsXCat|          202|          88515|     1|       73|                 73.0|\n",
      "|  PartidoPACMA|         1498|         232932|     1|       63|                 63.0|\n",
      "|  pablocasado_|         4567|         238926|     1|       50|                 50.0|\n",
      "|voxnoticias_es|         2146|          29582|     1|       44|                 44.0|\n",
      "|RaiLopezCalvet|         7579|          13574|     1|       43|                 43.0|\n",
      "|        iunida|        10225|         558318|     1|       39|                 39.0|\n",
      "|        Xuxipc|          311|         184967|     1|       37|                 37.0|\n",
      "|       Panik81|         1587|          15374|     1|       29|                 29.0|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = sqlContext.sql(\"\"\" SELECT user_agg.screen_name, user_agg.friends_count, user_agg.followers_count, user_agg.tweets,COUNT(*) as retweeted, COUNT(*) / user_agg.tweets AS ratio_tweet_retweeted\n",
    "                                FROM user_agg\n",
    "                                INNER JOIN tweets_sample\n",
    "                                WHERE user_agg.screen_name = retweeted_status.user.screen_name\n",
    "                                GROUP BY user_agg.screen_name, user_agg.friends_count, user_agg.followers_count, user_agg.tweets\n",
    "                                ORDER BY ratio_tweet_retweeted DESC\"\"\")\n",
    "\n",
    "retweeted.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ixMKO0CsGUG5"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Co-KvEGUG5"
   },
   "source": [
    "#### **Parte 1.2.2:** Queries a travÃ©s de la API\n",
    "\n",
    "Las tablas de Spark SQL ofrecen otro mecanismo para aplicar las transformaciones y obtener resultados similares a los que se obtendrÃ­a aplicando una consulta SQL. Por ejemplo utilizando el siguiente pipeline obtendremos el texto de todos los tweets en espaÃ±ol:\n",
    "\n",
    "```\n",
    "tweets_sample.where(\"lang == 'es'\").select(\"text\")\n",
    "```\n",
    "\n",
    "Que es equivalente a la siguiente sentencia SQL:\n",
    "\n",
    "```\n",
    "SELECT text\n",
    "FROM tweets_sample\n",
    "WHERE lang == 'es'\n",
    "```\n",
    "\n",
    "PodÃ©is consultar el [API de spark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html) para encontrar mÃ¡s informaciÃ³n sobre como utilitzar las diferentes transformaciones en tablas.\n",
    "\n",
    "En este ejercicio se os pide que repliquÃ©is la query obtenida en el apartado anterior empezando por generar la tabla ```users_agg```. PodÃ©is utilizar las transformaciones ```where```, ```select``` (o ```selectExpr```), ```groupBy```, ```count```, ```agg``` y ```orderBy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_IMzsk98GUG7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+--------------------+\n",
      "|    screen_name|max(friends_count)|count(screen_name)|max(followers_count)|\n",
      "+---------------+------------------+------------------+--------------------+\n",
      "|       anaoromi|              6258|                16|                6774|\n",
      "|    RosaMar6254|              6208|                14|                6245|\n",
      "|        lyuva26|              3088|                13|                3732|\n",
      "|PisandoFuerte10|              2795|                12|                1752|\n",
      "|     carrasquem|               147|                12|                 215|\n",
      "|       jasalo54|              1889|                11|                 689|\n",
      "|  PabloChabolas|              4925|                 9|                4042|\n",
      "|      lolalailo|              4922|                 9|                3738|\n",
      "|     Lordcrow11|              5002|                 9|                3069|\n",
      "|    DuroBelinda|              5242|                 9|                5778|\n",
      "+---------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = tweets_sample.where(\"user.lang == 'es'\").select(\"user.screen_name\",\"user.friends_count\",\"user.followers_count\")\n",
    "\n",
    "users_agg = users.groupBy(\"screen_name\")\\\n",
    "                 .agg({\"screen_name\":\"count\", \"followers_count\":\"max\", \"friends_count\":\"max\"})\\\n",
    "                 .orderBy(\"count(screen_name)\", ascending=False)\n",
    "\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGAzPr-MGUG7"
   },
   "source": [
    "Si os fijÃ¡is verÃ©is que el nombre de las columnas no corresponde con el obtenido anteriormente, podÃ©is cambiar el nombre de una columna determinada utilizando la transformaciÃ³n ```withColumnRenamed```. Cambiad el nombre de las columnas para que coincidan con el apartado anterior y guardadlas en una variable ```user_agg_new```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XrFU4iP6GUG7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  PabloChabolas|         4925|     9|           4042|\n",
      "|      lolalailo|         4922|     9|           3738|\n",
      "|     Lordcrow11|         5002|     9|           3069|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg_new = users_agg.withColumnRenamed(\"max(friends_count)\", \"friends_count\")\\\n",
    "                         .withColumnRenamed(\"count(screen_name)\", \"tweets\")\\\n",
    "                         .withColumnRenamed(\"max(followers_count)\", \"followers_count\")\n",
    "\n",
    "users_agg_new.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dgU_U8vWGUG7"
   },
   "outputs": [],
   "source": [
    "output = users_agg_new.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX3nUTbdGUG7"
   },
   "source": [
    "Cread ahora una tabla ```user_retweets``` utilizando transformaciones que contenga dos columnas:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***retweeted:*** nÃºmero de retweets\n",
    "\n",
    "PodÃ©is utilizar las mismas transformaciones que en el ejercicio anterior. Ordenad la tabla en orden descendente utilizando el valor de la columna ```retweeted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|    screen_name|retweeted|\n",
      "+---------------+---------+\n",
      "|         vox_es|      299|\n",
      "|  Santi_ABASCAL|      238|\n",
      "|   ahorapodemos|      238|\n",
      "|       iescolar|      166|\n",
      "|  AlbanoDante76|      161|\n",
      "|           PSOE|      155|\n",
      "| AntonioMaestre|      154|\n",
      "|         boye_g|      142|\n",
      "|   CiudadanosCs|      117|\n",
      "|Pablo_Iglesias_|      108|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_retweets = tweets_sample.select(\"retweeted_status.user.screen_name\", \"retweeted_status._id\") \\\n",
    "    .filter(\"retweeted_status.user.lang = 'es'\") \\\n",
    "    .groupBy(\"screen_name\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"retweeted\") \\\n",
    "    .orderBy(\"retweeted\", ascending=False)\n",
    "\n",
    "user_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Q8Idkd_jGUG8"
   },
   "outputs": [],
   "source": [
    "output = user_retweets.first()\n",
    "assert output.screen_name == 'vox_es' and output.retweeted == 299, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nmAyq16GUG8"
   },
   "source": [
    "Otra manera de combinar dos tablas es utilizando el [metodo de tabla ```join```](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html). Combinad la informaciÃ³n de la tabla ```users_agg_new``` y ```user_retweets``` en una nueva tabla ```retweeted``` utilizando la columna ```screen_name```. Ordenad la nueva tabla en orden descendente con el nombre de retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dLtDccNbGUG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "|   screen_name|friends_count|tweets|followers_count|   screen_name|retweeted|\n",
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "|          PSOE|        13635|     1|         671073|          PSOE|      155|\n",
      "|  CiudadanosCs|        92910|     1|         511896|  CiudadanosCs|      117|\n",
      "|     JuntsXCat|          202|     1|          88515|     JuntsXCat|       73|\n",
      "|  PartidoPACMA|         1498|     1|         232932|  PartidoPACMA|       63|\n",
      "|  pablocasado_|         4567|     1|         238926|  pablocasado_|       50|\n",
      "|voxnoticias_es|         2146|     1|          29582|voxnoticias_es|       44|\n",
      "|RaiLopezCalvet|         7579|     1|          13574|RaiLopezCalvet|       43|\n",
      "|        iunida|        10225|     1|         558318|        iunida|       39|\n",
      "|        Xuxipc|          311|     1|         184967|        Xuxipc|       37|\n",
      "|       Panik81|         1587|     1|          15374|       Panik81|       29|\n",
      "+--------------+-------------+------+---------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = users_agg_new.join(user_retweets,users_agg_new.screen_name == user_retweets.screen_name, 'inner')\\\n",
    "                         .orderBy(\"retweeted\", ascending = False)\n",
    "\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BPI1fQ8QGUG8"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmdB3_lNGUG8"
   },
   "source": [
    "NotarÃ©is que algunos de los registros que aparecen en la tabla ```users_retweeted``` no estÃ¡n presentes en la tabla retweeted. Esto es debido a que, por defecto, el mÃ©todo aplica un inner join y por tanto solo combina los registros presentes en ambas tablas. PodÃ©is cambiar este comportamiento a travÃ©s de los parÃ¡metros de la funciÃ³n.\n",
    "\n",
    "Para terminar esta parte y reconstruir el resultado del apartado 1.2.1 vamos a aÃ±adir una columna ```ratio_tweet_retweeted``` con informaciÃ³n del ratio entre retweets y tweets. Para ello debÃ©is utilizar la transformaciÃ³n ```withColumn```. El resultado debe estar ordenado considerando esta nueva columna en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aZPkN3TpGUG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "|   screen_name|friends_count|tweets|followers_count|   screen_name|retweeted|ratio_tweet_retweeted|\n",
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "|          PSOE|        13635|     1|         671073|          PSOE|      155|                155.0|\n",
      "|  CiudadanosCs|        92910|     1|         511896|  CiudadanosCs|      117|                117.0|\n",
      "|     JuntsXCat|          202|     1|          88515|     JuntsXCat|       73|                 73.0|\n",
      "|  PartidoPACMA|         1498|     1|         232932|  PartidoPACMA|       63|                 63.0|\n",
      "|  pablocasado_|         4567|     1|         238926|  pablocasado_|       50|                 50.0|\n",
      "|voxnoticias_es|         2146|     1|          29582|voxnoticias_es|       44|                 44.0|\n",
      "|RaiLopezCalvet|         7579|     1|          13574|RaiLopezCalvet|       43|                 43.0|\n",
      "|        iunida|        10225|     1|         558318|        iunida|       39|                 39.0|\n",
      "|        Xuxipc|          311|     1|         184967|        Xuxipc|       37|                 37.0|\n",
      "|       Panik81|         1587|     1|          15374|       Panik81|       29|                 29.0|\n",
      "+--------------+-------------+------+---------------+--------------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = retweeted.withColumn(\"ratio_tweet_retweeted\", retweeted[\"retweeted\"] / retweeted[\"tweets\"]).orderBy(\"ratio_tweet_retweeted\", ascending=False)\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pKmQvo-dGUG9"
   },
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QvvaxglGUG9",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 2:** Bases de datos HIVE y operaciones complejas\n",
    "\n",
    "Hasta ahora hemos estado trabajando con un pequeÃ±o sample de los tweets generados (el 0.1%). En esta parte de la PEC vamos a ver como trabajar y tratar con el dataset completo. Para ello vamos a utilizar tanto transformaciones sobre tablas como operaciones sobre RDD cuando sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSfN7McGUG9"
   },
   "source": [
    "### **Parte 2.1:** Bases de datos Hive\n",
    "\n",
    "Muchas veces los datos con los que vamos a trabajar se van a utilizar en diversos proyectos. Una manera de organizar los datos es, en lugar de utilizar directamente los ficheros, recurrir a una base de datos para gestionar la informaciÃ³n. En el entorno Hadoop una de las bases de datos mÃ¡s utilizadas es [Apache Hive](https://hive.apache.org/), una base de datos que permite trabajar con contenido distribuido.\n",
    "\n",
    "La manera de acceder a esta base de datos es creando un contexto Hive de manera muy similar a como declaramos un contexto SQL. Primero de todo vamos a declarar un variable ```hiveContext``` instanciÃ¡ndola como un objeto de la classe ```HiveContext```. Acto seguido vamos a comprobar cuantas tablas estÃ¡n registradas en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4uUhHlKXGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "| default|               as|      false|\n",
      "| default|        mistweets|      false|\n",
      "| default|     province_28a|      false|\n",
      "| default|province_28a_vext|      false|\n",
      "| default|              src|      false|\n",
      "| default|        tweets28a|      false|\n",
      "| default|   tweets28a_vext|      false|\n",
      "| default|        user_info|      false|\n",
      "| default|   user_info_vext|      false|\n",
      "|        |    tweets_sample|       true|\n",
      "|        |         user_agg|       true|\n",
      "+--------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext = HiveContext(sc)\n",
    "hiveContext.tables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT5Lzi2dGUG9"
   },
   "source": [
    "Observad que ahora mismo tenemos cinco tablas registradas en este contexto. Tres de ellas no temporales y dos temporales, las que hemos registrado previamente. Por tanto sqlContext y hiveContext estÃ¡n concetados (es la misma sessiÃ³n)\n",
    "\n",
    "Vamos a crear una variable ```tweets``` que utilizaremos para acceder a la tabla ```tweets28a``` guardada en ```hiveContext``` utilizando para ello el mÃ©todo ```table()``` de este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "psEvAQUEGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 25419835 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = hiveContext.table(\"tweets28a\")\n",
    "print(\"Loaded dataset contains {} tweets\".format(tweets.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGlfGwuHGUG9"
   },
   "source": [
    "Utilizando el mismo mÃ©todo que en el apartado 1.1, comprobad la estructura de la tabla que acabamos de cargar con ```printSchema()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SC61HsFsGUG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTDFYQzlGUG-",
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 2.2:** MÃ¡s allÃ¡ de las transformaciones SQL\n",
    "\n",
    "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van mÃ¡s allÃ¡ de lo que podemos conseguir utilizando el lenguaje SQL. En esta parte de la prÃ¡ctica vamos practicar cÃ³mo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyzhlfUFGUG-",
    "toc-hr-collapsed": false
   },
   "source": [
    "#### **Parte 2.2.1:** Tweets por poblaciÃ³n\n",
    "\n",
    "Un pequeÃ±o porcentaje, alrededor del 1%, de los tweets realizados estÃ¡ geolocalizado. Eso quiere decir que para estos tweets tenemos informaciÃ³n acerca del lugar donde han sido realizados guardado en el campo ```place```. En este ejercicio se pide que utilizando una sentencia SQL mostrÃ©is en orden descendente cuÃ¡ntos tweets se han realizado en cada lugar. La tabla resultante ```tweets_place``` debe tener las siguientes columnas:\n",
    "- ***name:*** nombre del lugar\n",
    "- ***tweets:*** nÃºmero de tweets\n",
    "\n",
    "Recordad que no todos los tweets en la base de datos tienen que tener informaciÃ³n geolocalizada, tenÃ©is que filtrarlos teniendo en cuenta todos los que tienen un valor no nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kOMIut4rGUG-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|VillamartÃ­n|  2364|\n",
      "|     MÃ¡laga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = hiveContext.sql(\"\"\"  SELECT place.name, Count (*) as tweets\n",
    "                                    FROM tweets28a\n",
    "                                    WHERE place IS NOT NULL\n",
    "                                    GROUP BY place.name\n",
    "                                    ORDER BY tweets DESC\"\"\")\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nbT2HdsMGUG-"
   },
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hTeGFtnGUG-"
   },
   "source": [
    "#### **Parte 2.2.2:** Contar hashtags\n",
    "\n",
    "Como seguro que habÃ©is observado la semÃ¡ntica de la sentencia SQL es mucho mÃ¡s limpia que trabajar con RDDs para realizar muchas tareas, pero no todas las que os vais a encontrar se pueden hacer mediante sentencias SQL. En este ejercicio vamos a ver un ejemplo.\n",
    "\n",
    "El objetivo de este ejercicio es contar el nÃºmero de veces que cada hashtag (palabras precedidas por un #) ha aparecido en el dataset. Para evitar la sobrerrepresentaciÃ³n debida a los retweets vamos a concentrarnos en solo aquellos tweets que no son retweets de ningÃºn otro, o dicho de otra manera, en aquellos en los que el campo ```retweeted_status``` es nulo. Cread una variable ```non_retweets``` que contenga todos estos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Gwon1opoGUG-"
   },
   "outputs": [],
   "source": [
    "non_retweets = tweets.where(\"retweeted_status IS NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avG-xtDqGUG_"
   },
   "source": [
    "Seguidamente vamos a crear una variable ```hashtags``` que contenga una lista de tuplas con la informaciÃ³n ```(hashtag, count)```. Para ello, cread un RDD que contenga una lista con el texto de todos los tweets. Una vez hecho este paso tenÃ©is que extraer los hashtags (palabras precedidas por un #) y contarlos.\n",
    "\n",
    "Recordad los conocimientos adquiridos en la PEC 'word count' y el anterior ejercicio, os serÃ¡n de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A0S68bKCGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#039', 278), ('#laEspaÃ±aquequiero', 2), ('#En', 1), ('#checknews', 1), ('#elizabethwarren', 6), ('#amrkplaying', 1), ('#BADAJOZ', 1), ('#ThisistheRealSpain', 1), ('#doodle', 3), ('#VotadInsensatos', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Puede modificarse si se considera conveniente el map y el flatMap\n",
    "import re\n",
    "\n",
    "hashtags = non_retweets.rdd.map(lambda row: row.text) \\\n",
    "                           .flatMap(lambda line: re.findall(r\"#\\w+\", line))\n",
    "\n",
    "hashtags = hashtags.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(hashtags.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciMb5GI4GUG_"
   },
   "source": [
    "Finalmente, se os pide que con el RDD obtenido generÃ©is una tabla ```hashtagsTable``` compuesta de dos columnas:\n",
    "- ***hashtag***\n",
    "- ***num:*** nÃºmero de veces que aparece cada hashtag.\n",
    "\n",
    "Ordenadla en orden descendente por nÃºmero de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XV_NgL0oGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             hashtag|   num|\n",
      "+--------------------+------+\n",
      "|                #28A|158133|\n",
      "|   #ElDebateDecisivo|108607|\n",
      "|     #ELDEBATEenRTVE| 94250|\n",
      "|#EleccionesGenera...| 33815|\n",
      "|     #EquiparacionYa| 30567|\n",
      "|       #EleccionesL6| 30075|\n",
      "|         #HazQuePase| 26526|\n",
      "|   #DebateAtresmedia| 21854|\n",
      "|         #DebateRTVE| 17711|\n",
      "|#LaHistoriaLaEscr...| 16964|\n",
      "|          #PorEspaÃ±a| 16169|\n",
      "|           #DebatTV3| 14705|\n",
      "|#EleccionesGenerales| 14441|\n",
      "|                #28a| 14327|\n",
      "|         #ILPJusapol| 13488|\n",
      "|            #28Abril| 12848|\n",
      "|         #EspaÃ±aViva| 12620|\n",
      "|           #VotaPSOE| 12282|\n",
      "|        #ValorSeguro| 11149|\n",
      "|             #EspaÃ±a| 10753|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir RDD a DataFrame\n",
    "hashtagsTable = hashtags.map(lambda row: Row(hashtag=row[0], num=row[1])).toDF().orderBy(\"num\", ascending=False)\n",
    "\n",
    "\n",
    "# Primeras 20 lÃ­neas\n",
    "hashtagsTable.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HfsRziG9GUG_"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-80fb386d50cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashtagsTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#28A\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m158124\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output"
     ]
    }
   ],
   "source": [
    "output = hashtagsTable.first()\n",
    "assert output.hashtag == \"#28A\" and output.num == 158124, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "907RhzRjGUG_",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 3:** Sampling\n",
    "\n",
    "En muchas ocasiones, antes de lanzar costoso procesos, es una prÃ¡ctica habitual tratar con un pequeÃ±o conjunto de los datos para investigar algunas propiedades o simplemente para debugar nuestros algoritmos, a esta tarea se la llama sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iVwpgsRGUG_"
   },
   "source": [
    "El sampling aplicado es [el homogeneo](https://en.wikipedia.org/wiki/Simple_random_sample). Este sampling se basta en simplemente escoger una fracciÃ³n de la poblaciÃ³n seleccionando aleatoriamente elementos de la misma.\n",
    "\n",
    "Primero de todo vamos ha realizar un sampling homogÃ©neo del 1% de los tweets generados en periodo electoral sin reemplazo. Guardad en una variable ```tweets_sample``` este sampling utilizando el mÃ©todo ```sample``` descrito en la [API de pyspark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html). El seed que vais a utilizar para inicializar el generador aleatorio es 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4jPfvfgEGUG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets sampled: 254364\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "tweets_sample = tweets.sample(False, fraction, seed)\n",
    "\n",
    "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "w-TJslJ6GUHA"
   },
   "outputs": [],
   "source": [
    "assert tweets_sample.count() == 254364, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaQedjXSGUHA"
   },
   "source": [
    "Una de las cosas que resulta interesante comprobar acerca de los patrones de uso de las redes sociales es el patrÃ³n de uso diario. En este caso nos interesa el nÃºmero promedio de tweets que se genera cada hora del dÃ­a. Para extraer esta informaciÃ³n lo que haremos primero, serÃ¡ generar una tabla ```tweets_timestamp``` con la informaciÃ³n:\n",
    "- ***created_at***: timestamp de cuando se publicÃ³ el tweet.\n",
    "- ***hour***: a que hora del dia corresponde.\n",
    "- ***day***: Fecha en formato MM-dd-YY\n",
    "\n",
    "La fecha que figura en la base de datos esta en la franja horaria GMT. El primer paso es pasar esta informaciÃ³n al horario peninsular de EspaÃ±a, podÃ©is utilizar la funciÃ³n ```from_utc_timestamp``` para este fin. AsÃ­ mismo, la funciÃ³n ```hour``` os servirÃ¡ para extraer la hora del timestamp y la funciÃ³n ```date_format``` os permitirÃ¡ generar la fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YQKaC5yAGUHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----------------+\n",
      "|         created_at|hour|tweets_timestamp|\n",
      "+-------------------+----+----------------+\n",
      "|2019-04-13 22:57:00|  22|        04-13-19|\n",
      "|2019-04-13 22:57:05|  22|        04-13-19|\n",
      "|2019-04-13 22:57:06|  22|        04-13-19|\n",
      "|2019-04-13 22:57:27|  22|        04-13-19|\n",
      "|2019-04-13 22:57:28|  22|        04-13-19|\n",
      "|2019-04-13 22:57:30|  22|        04-13-19|\n",
      "|2019-04-13 22:57:37|  22|        04-13-19|\n",
      "|2019-04-13 22:57:56|  22|        04-13-19|\n",
      "|2019-04-13 22:58:07|  22|        04-13-19|\n",
      "|2019-04-13 22:58:09|  22|        04-13-19|\n",
      "|2019-04-13 22:58:11|  22|        04-13-19|\n",
      "|2019-04-13 22:58:12|  22|        04-13-19|\n",
      "|2019-04-13 22:58:16|  22|        04-13-19|\n",
      "|2019-04-13 22:58:22|  22|        04-13-19|\n",
      "|2019-04-13 22:58:28|  22|        04-13-19|\n",
      "|2019-04-13 22:58:36|  22|        04-13-19|\n",
      "|2019-04-13 22:58:38|  22|        04-13-19|\n",
      "|2019-04-13 22:58:42|  22|        04-13-19|\n",
      "|2019-04-13 22:58:46|  22|        04-13-19|\n",
      "|2019-04-13 22:58:51|  22|        04-13-19|\n",
      "+-------------------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, hour, from_utc_timestamp\n",
    "\n",
    "tweets_timestamp = tweets_sample.select(\"created_at\")\n",
    "tweets_timestamp = tweets_timestamp.withColumn(\"created_at\", from_utc_timestamp(\"created_at\", \"GMT\"))\n",
    "tweets_timestamp = tweets_timestamp.withColumn(\"hour\", hour(\"created_at\"))\\\n",
    "                                   .withColumn(\"tweets_timestamp\", date_format(\"created_at\", \"MM-dd-YY\"))\n",
    "tweets_timestamp.limit(20).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1SndGF7GUHA"
   },
   "source": [
    "El paso siguiente es agregar estos datos por hora y dÃ­a en una tabla ```tweets_hour_day```. TenÃ©is que crear una tabla ```tweets_hour``` con la informaciÃ³n:\n",
    "- ***hour:*** hora del dia\n",
    "- ***day:*** fecha\n",
    "- ***count:*** nÃºmero de tweets generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6DlTy-B2GUHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----+\n",
      "|hour|tweets_timestamp|count|\n",
      "+----+----------------+-----+\n",
      "|  18|        04-13-19|  525|\n",
      "|  11|        04-17-19|  562|\n",
      "|  10|        04-28-19|  774|\n",
      "|  23|        04-23-19| 4877|\n",
      "|  11|        04-28-19|  993|\n",
      "|   7|        04-22-19|  207|\n",
      "|  16|        04-24-19|  971|\n",
      "|   5|        04-28-19|   78|\n",
      "|   6|        04-29-19|  267|\n",
      "|   4|        04-17-19|   70|\n",
      "|   5|        04-27-19|  108|\n",
      "|  10|        04-14-19|  487|\n",
      "|  23|        04-26-19| 1201|\n",
      "|   7|        04-17-19|  267|\n",
      "|  13|        04-25-19|  734|\n",
      "|  20|        04-28-19| 1587|\n",
      "|   0|        04-23-19| 2808|\n",
      "|   6|        04-23-19|  245|\n",
      "|   7|        04-26-19|  392|\n",
      "|   8|        04-21-19|  269|\n",
      "+----+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_hour_day = tweets_timestamp.groupBy(tweets_timestamp.hour, tweets_timestamp.tweets_timestamp).count()\n",
    "\n",
    "tweets_hour_day.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMhAmufaGUHA"
   },
   "source": [
    "Por Ãºltimo solo nos queda hacer una agregaciÃ³n por hora para conseguir el promedio de tweets por hora. TenÃ©is que generar una tabla ```tweets_hour``` con la informaciÃ³n:\n",
    "- ***hour:*** Hora\n",
    "- ***tweets:*** Promedio de tweets realizados\n",
    "\n",
    "Recordad que estamos trabajando con un sample del 1% por tanto tenÃ©is que corregir la columna ```tweets``` para que refleje el promedio que deberÃ­amos esperar en el conjunto completo de tweets. La tabla tiene que estar ordenada en orden ascendente de hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0i34Gpq7GUHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|            tweets|\n",
      "+----+------------------+\n",
      "|   0|104822.22222222222|\n",
      "|   1|51744.444444444445|\n",
      "|   2| 23063.15789473684|\n",
      "|   3|12505.555555555555|\n",
      "|   4| 8372.222222222223|\n",
      "|   5|  8264.70588235294|\n",
      "|   6|11977.777777777777|\n",
      "|   7|26455.555555555555|\n",
      "|   8| 40538.88888888889|\n",
      "|   9|54717.647058823524|\n",
      "|  10|           58550.0|\n",
      "|  11| 61761.11111111111|\n",
      "|  12|62266.666666666664|\n",
      "|  13| 66105.55555555555|\n",
      "|  14| 66816.66666666666|\n",
      "|  15| 70644.44444444444|\n",
      "|  16| 69005.55555555555|\n",
      "|  17| 64538.88888888889|\n",
      "|  18| 66616.66666666666|\n",
      "|  19| 67922.22222222222|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "tweets_hour = tweets_hour_day.groupBy(\"hour\").agg(avg(\"count\").alias(\"tweets\")).orderBy(\"hour\").withColumnRenamed(\"avg(tweets_hour_day)\", \"tweets\")\n",
    "             \n",
    "tweets_hour = tweets_hour.withColumn(\"tweets\", tweets_hour.tweets / fraction)\n",
    "tweets_hour.limit(24).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB4vQ-xkGUHC"
   },
   "source": [
    "Por Ãºltimo, tenÃ©is que producir un grÃ¡fico de barras utilizando Pandas (pasamos el dataframe a Pandas) donde se muestre la informaciÃ³n que acabÃ¡is de generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2iw6XjQ2GUHD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8a989a48d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_hour_pd = tweets_hour.toPandas()\n",
    "tweets_hour_pd.plot.bar(x=\"hour\", y=\"tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-VtaqK2GUHD",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 4:** IntroducciÃ³n a los datos relacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6v0z-w0GUHD"
   },
   "source": [
    "El hecho de trabajar con una base de datos que contiene informaciÃ³n generada en una red social nos permite introducir el concepto de datos relacionales. Podemos definir datos relacionales como aquellos en los que existen relaciones entre las entidades que constituyen la base de datos. Si estas relaciones son binarias, relaciones 1 a 1, podemos representar las relaciones como un grafo compuesto por un conjunto de vÃ©rtices $\\mathcal{V}$ y un conjunto de aristas $\\mathcal{E}$ que los relacionan.\n",
    "\n",
    "En el caso de grafos que emergen de manera orgÃ¡nica, este tipo de estructura va mÃ¡s allÃ¡ de los grafos regulares que seguramente conocÃ©is. Este tipo de estructuras se conocen como [redes complejas](https://es.wikipedia.org/wiki/Red_compleja). El estudio de la estructura y dinÃ¡micas de este tipo de redes ha contribuido a importantes resultados en campos tan dispares como la fÃ­sica, la sociologÃ­a, la ecologÃ­a o la medicina.\n",
    "\n",
    "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
    "\n",
    "En esta Ãºltima parte de la prÃ¡ctica vamos a trabajar con este tipo de datos. En concreto vamos a modelar uno de los posibles relaciones presentes en el dataset, la red de retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj3QBdQXGUHD"
   },
   "source": [
    "### Generar la red de retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inpex3H9GUHD"
   },
   "source": [
    "#### **Parte 4.1**: ConstrucciÃ³n de la edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOA8W2sGUHD"
   },
   "source": [
    "Lo primero se os pide es que generÃ©is la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teÃ³rico, la manera mÃ¡s habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta prÃ¡ctica vamos a centrarnos en el aspecto computacional, una de las maneras de mas eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relaciÃ³n a parejas entre las entidades.\n",
    "\n",
    "Las relaciones pueden ser bidireccionales o direccionales y tener algÃºn peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuÃ¡ntas veces esto ha pasado.\n",
    "\n",
    "Lo primero que harÃ©is para simplificar el cÃ³mputo,  es crear un sample homogÃ©neo sin reemplazo del 1% de los tweets. Utilizando los conocimientos que habÃ©is aprendido en el apartado 3.1. Utilizaremos 42 como valor para la seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Vxahz1fJGUHD"
   },
   "outputs": [],
   "source": [
    "sample = tweets.sample(False, 0.01, 42).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwojQwBIGUHD"
   },
   "source": [
    "Ahora vais a crear una tabla ```edgelist``` con la siguiente informaciÃ³n:\n",
    "- ***src:*** usuario que retuitea\n",
    "- ***dst:*** usuario que es retuiteado\n",
    "- ***weight:*** nÃºmero de veces que un usuario retuitea a otro.\n",
    "\n",
    "Filtrar el resultado para que contenga sÃ³lo las relaciones con un weight igual o mayor a dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wImuemZDGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5243 edges on the network.\n"
     ]
    }
   ],
   "source": [
    "edgelist = sample.where(\"retweeted_status IS NOT NULL\")\\\n",
    "                 .selectExpr(\"user.screen_name AS src\", \"retweeted_status.user.screen_name AS dst\")\n",
    "\n",
    "edgelist = edgelist.groupBy(\"src\", \"dst\").count().withColumnRenamed(\"count\", \"weight\").where(\"weight >= 2\")\n",
    "\n",
    "L = edgelist.count()\n",
    "\n",
    "print(\"There are {0} edges on the network.\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bbkDixqiGUHE"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect ouput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-94f5e6f4b351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5247\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect ouput\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect ouput"
     ]
    }
   ],
   "source": [
    "assert L == 5247, \"Incorrect ouput\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCehZGLWGUHE"
   },
   "source": [
    "#### **Parte 4.2:** Centralidad de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XvkaBHFGUHE"
   },
   "source": [
    "Uno de los descriptores mÃ¡s comunes en el anÃ¡lisis de redes es el grado. El grado cuantifica cuÃ¡ntas aristas estÃ¡n conectadas a cada vÃ©rtices. En el caso de redes dirigidas como la que acabamos de crear este descriptor estÃ¡ descompuesto en el:\n",
    "- **in degree**: cuantas aristas apuntan al nodo\n",
    "- **out degree**: cuantas aristas salen del nodo\n",
    "\n",
    "Si haces un ranquing de estos valores vais a obtener medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
    "\n",
    "Se os pide que generÃ©is una tabla con la informaciÃ³n:\n",
    "- ***screen_name:*** nombre del usuario (anteriormene src).\n",
    "- ***outDegree:*** out degree del nodo.\n",
    "\n",
    "Ordenado la tabla por out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tTfZfEOGGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|            src|outDegree|\n",
      "+---------------+---------+\n",
      "|   rosavergar23|       12|\n",
      "|     carrasquem|       10|\n",
      "|Teresaperezcep1|       10|\n",
      "|  joanagabarrof|        9|\n",
      "|MarcoCostaValue|        9|\n",
      "|     pallaron12|        9|\n",
      "|   Socialista60|        8|\n",
      "|    SSarelvis67|        8|\n",
      "|JulioAl18175505|        8|\n",
      "|       jasalo54|        8|\n",
      "|MariaJo40891027|        8|\n",
      "|   Pepa63306772|        7|\n",
      "|    RosaMar6254|        7|\n",
      "|     Nacho_JISF|        7|\n",
      "|     nusagatero|        7|\n",
      "|      ISMVALSyR|        7|\n",
      "|    mariaje1956|        7|\n",
      "|      el_partal|        7|\n",
      "|        lyuva26|        7|\n",
      "|  PabloChabolas|        6|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "outDegree = edgelist.groupBy(\"src\").agg({\"dst\": \"count\"}).withColumnRenamed(\"count(dst)\", \"outDegree\").orderBy(\"outDegree\", ascending=False)\n",
    "\n",
    "outDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "6jvwDBaPGUHE"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "screen_name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0;31m# but this will not be used in normal cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'screen_name' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6bf158fb7e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutDegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rosavergar23\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutDegree\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: screen_name"
     ]
    }
   ],
   "source": [
    "output = outDegree.first()\n",
    "assert output.screen_name == \"rosavergar23\" and output.outDegree == 11, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAWf7I9CGUHE"
   },
   "source": [
    "Se os pide ahora que generÃ©is una tabla con la informaciÃ³n:\n",
    "- ***screen_name:*** nombre del usuario (anteriormente dst).\n",
    "- ***inDegree:*** in degree del nodo.\n",
    "\n",
    "Ordenad la tabla por in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "erRzYC6MGUHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|            dst|inDegree|\n",
      "+---------------+--------+\n",
      "|         vox_es|     321|\n",
      "|   ahorapodemos|     275|\n",
      "|           PSOE|     256|\n",
      "|   CiudadanosCs|     202|\n",
      "|  Santi_ABASCAL|     200|\n",
      "|      populares|     133|\n",
      "|  AlbanoDante76|     105|\n",
      "|           KRLS|      93|\n",
      "|Front_Republica|      90|\n",
      "|Pablo_Iglesias_|      80|\n",
      "|       ivanedlm|      70|\n",
      "|       iescolar|      70|\n",
      "|sanchezcastejon|      69|\n",
      "|         boye_g|      57|\n",
      "| AntonioMaestre|      56|\n",
      "|      JuntsXCat|      56|\n",
      "| hermanntertsch|      55|\n",
      "|     eldiarioes|      46|\n",
      "|   pablocasado_|      40|\n",
      "|    CastigadorY|      39|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replicando el cÃ³digo utilizado para generar el outDegree, generad el inDegree\n",
    "inDegree = edgelist.groupBy(\"dst\").agg({\"src\": \"count\"}).withColumnRenamed(\"count(src)\", \"inDegree\").orderBy(\"inDegree\", ascending=False)\n",
    "\n",
    "inDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQzI17PJGUHF"
   },
   "outputs": [],
   "source": [
    "output = inDegree.first()\n",
    "assert output.screen_name == \"vox_es\" and output.inDegree == 330, \"Incorrect output\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S4Co-KvEGUG5",
    "SyzhlfUFGUG-",
    "8hTeGFtnGUG-",
    "Inpex3H9GUHD",
    "zCehZGLWGUHE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
